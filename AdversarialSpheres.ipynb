{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "import dataset\n",
    "import model\n",
    "import utils\n",
    "import attacks\n",
    "\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphereset = dataset.sphere_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = model.LargeReLU(sphere_dim=500,\n",
    "                 n_hidden=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks\n",
    "\n",
    "1. For an arbitrary input, the model should output a number very close to zero, since it's the output of the final linear layer, which should be initialized with mean 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 12900742, Output: tensor([-0.0038], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "#Check for a single input at a time\n",
    "\n",
    "idx = np.random.randint(0, len(sphereset))\n",
    "print('Index: {}, Output: {}'.format(idx, net1(sphereset[idx][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram_results = utils.histogram_outputs(net1, 500)\n",
    "plt.bar(histogram_results[1], histogram_results[0], width = 1./30)\n",
    "plt.xlabel('Model output')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(-0.5,0.5)\n",
    "plt.title('Model outputs before training');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initially, the accuracy of the model should be a half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.eval_accuracy(net1, n_pts=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-bf135b07e979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "batch_size = 50\n",
    "dataloader = torch.utils.data.DataLoader(sphereset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(net1.parameters(), lr = 0.0001)\n",
    "\n",
    "net1.train()\n",
    "\n",
    "losses = np.zeros(1000000)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "checkpoint_steps = [10, 20,500, 1000, 2000, 3000, 4000, 5000, 10000, 100000] \n",
    "\n",
    "for i, (points, labels) in enumerate(dataloader):\n",
    "    \n",
    "    net1.zero_grad()\n",
    "    \n",
    "    output = net1(points)\n",
    "    \n",
    "    loss = loss_fun(output, labels.view(-1,1).float())\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    losses[i] = loss.item()\n",
    "    \n",
    "    if i in checkpoint_steps:\n",
    "        torch.save({'iteration': i, \n",
    "                    'model_state_dict': net1.state_dict(),\n",
    "                    'loss': loss.item()}, 'trained_models/largeReLU_{}_{}.pth'.format(1, i))\n",
    "        \n",
    "    \n",
    "    if i > 1000000:\n",
    "        break\n",
    "    # should not be necessary, because the dataset only has 50 million images\n",
    "    \n",
    "print('Elapsed time: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'iteration': i, \n",
    "                    'model_state_dict': net1.state_dict(),\n",
    "                    'loss': loss.item()}, 'trained_models/largeReLU_{}_{}.pth'.format(1, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb7d2c4a198>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmcHVWd9/HPz8TgNiBL9GESZhI1\nz4wZGRFixHFkRkUMLoRnBsYgI+jI8IDDozOMSxhHcAIiqEMQjUhklyUkEYYIgZCVJYQknX3tpJM0\nSWftbJ2109vv+ePWTW7fvt237l51+/t+vfrV91adqnvqVt36VZ1z6hxzd0RERN5S6QyIiEg0KCCI\niAiggCAiIgEFBBERARQQREQkoIAgIiKAAoKIiAQUEEREBFBAEBGRQN9KZyAXZ5xxhg8aNKjS2RAR\niZVFixbtdvf+2dLFKiAMGjSImpqaSmdDRCRWzOzNMOlUZCQiIoACgoiIBEIFBDMbYWa1ZlZnZqMz\nzL/RzFab2XIzm2lmf5oy72ozWx/8XZ0y/TwzWxGs8x4zs+JskoiI5CNrQDCzPsA44GJgKHCFmQ1N\nS7YEGObufwlMBn4aLHsacAvwMWA4cIuZnRoscy9wLTAk+BtR8NaIiEjewtwhDAfq3H2ju7cAE4CR\nqQncfba7HwnevgEMDF5/Dpju7nvdfR8wHRhhZmcCJ7v7PE8MyPAocGkRtkdERPIUJiAMALakvG8I\npnXnG8ALWZYdELzOuk4zu9bMasysprGxMUR2RUQkH2ECQqay/YzDrJnZPwLDgJ9lWTb0Ot19vLsP\nc/dh/ftnbUYrIiJ5ChMQGoCzUt4PBLalJzKzC4EfAJe4+7EsyzZwolip23WKyAlt7R1MrNlCR4eG\nvZXSCBMQFgJDzGywmfUDRgFTUhOY2UeA+0gEg10ps6YBF5nZqUFl8kXANHffDhw0s/OD1kVXAc8W\nYXtEqtb9r23ie5OXM2nRluyJRfKQ9Ulld28zsxtInNz7AA+6+yozGwPUuPsUEkVE7wImBa1HN7v7\nJe6+18xuJRFUAMa4+97g9fXAw8DbSdQ5vICIdGvv4RYA9h9prXBOpFqF6rrC3acCU9Om3Zzy+sIe\nln0QeDDD9BrgQ6FzKiIiJaUnlUVEBFBAEBGRgAKCiIgACggiIhJQQBAREUABQUREAgoIIiICKCCI\niEhAAUHKav3OgyzYtDd7QhEpu1BPKosUy2fHvgJA/R1fqHBORCSd7hBEYkZ9nUqpKCCIxIQGHZdS\nU0AQERFAAUFERAIKCCIiAiggiIhIIFRAMLMRZlZrZnVmNjrD/AvMbLGZtZnZZSnTP2VmS1P+ms3s\n0mDew2a2KWXeOcXbLBERyVXW5xDMrA8wDvgs0AAsNLMp7r46Jdlm4GvAd1KXdffZwDnBek4D6oCX\nUpJ8190nF7IBIiJSHGEeTBsO1Ln7RgAzmwCMBI4HBHevD+Z19LCey4AX3P1I3rkVEZGSCVNkNADY\nkvK+IZiWq1HAk2nTfmxmy81srJmdlMc6RUSkSMIEhEzPw+T0sKSZnQmcDUxLmXwT8OfAR4HTgO93\ns+y1ZlZjZjWNjY25fKyIiOQgTEBoAM5KeT8Q2Jbj5/wD8Iy7tyYnuPt2TzgGPESiaKoLdx/v7sPc\nfVj//v1z/FiR6qEuK6TUwgSEhcAQMxtsZv1IFP1MyfFzriCtuCi4a8DMDLgUWJnjOkV6JXVhIaWS\nNSC4extwA4ninjXARHdfZWZjzOwSADP7qJk1AJcD95nZquTyZjaIxB3Gy2mrftzMVgArgDOA2wrf\nHJHqpzsFKZVQ3V+7+1Rgatq0m1NeLyRRlJRp2XoyVEK7+6dzyahIb6c7Ayk1PaksIiKAAoKIiAQU\nEEREBFBAqFpvbNzDCyu2VzobIhIjGlO5So0a/wagsYtFJDzdIYiICKCAICIiAQUEEREBFBBERCSg\ngCAiIoACgoiIBBQQRGLG1budlIgCgkhcqHc7KTEFBBERARQQREQkoIAgvc7qbQf4/C9e5dCxtkpn\nRSRSQgUEMxthZrVmVmdmozPMv8DMFptZm5ldljav3cyWBn9TUqYPNrP5ZrbezJ4KhucUKbk7X1zL\n6u0HWFi/t9JZEYmUrAHBzPoA44CLgaHAFWY2NC3ZZuBrwBMZVnHU3c8J/i5JmX4nMNbdhwD7gG/k\nkX8RESmSMHcIw4E6d9/o7i3ABGBkagJ3r3f35UBHmA81MwM+DUwOJj0CXBo61yISyuzaXUxZtq3S\n2ZCYCBMQBgBbUt43kGGM5B68zcxqzOwNM0ue9E8H9rt7shA313VKlWlubWdHU3Ols1F1vv7QQr71\n5JJKZ0NiIkxAyNT6OZdHY/7E3YcBXwHuNrP357JOM7s2CCg1jY2NOXysxMlVDy7g/J/MrHQ2RHq1\nMAGhATgr5f1AIPQ9qLtvC/5vBOYAHwF2A+82s+QAPd2u093Hu/swdx/Wv3//sB8rMbNgkyp4s9IT\nylJiYQLCQmBI0CqoHzAKmJJlGQDM7FQzOyl4fQbwCWC1uzswG0i2SLoaeDbXzIv0RqYnlqVEsgaE\noJz/BmAasAaY6O6rzGyMmV0CYGYfNbMG4HLgPjNbFSz+QaDGzJaRCAB3uPvqYN73gRvNrI5EncID\nxdwwERHJTagxld19KjA1bdrNKa8Xkij2SV/udeDsbta5kUQLJhHJgTq3k1LRk8oicaGiIikxBQTp\nvXSlLdKJAoL0OqqUFclMAUFERAAFhKpXow7cRCQkBYQqd9lv5lU6CyISEwoIIiICKCCIiEhAAUFE\nRAAFBBERCSggSK+jrh9EMlNAEBERQAFBeiE9qSyl1tLWwXcnLYvdKIAKCCIx4+qEKfJmrd3FpEUN\n3PzsykpnJScKCCIxYeruVEpMAUFERICQAcHMRphZrZnVmdnoDPMvMLPFZtZmZpelTD/HzOaZ2Soz\nW25mX06Z97CZbTKzpcHfOcXZJBERyUfWEdPMrA8wDvgs0AAsNLMpKUNhAmwGvgZ8J23xI8BV7r7e\nzP4YWGRm09x9fzD/u+4+udCNEBGRwoUZQnM4UBcMeYmZTQBGAscDgrvXB/M6Uhd093Upr7eZ2S6g\nP7AfERGJlDBFRgOALSnvG4JpOTGz4UA/YEPK5B8HRUljzeykXNcpIiLFEyYgZGrakFO7NzM7E/gd\n8HV3T95F3AT8OfBR4DTg+90se62Z1ZhZTWNjYy4fKyIiOQgTEBqAs1LeDwS2hf0AMzsZeB74T3d/\nIznd3bd7wjHgIRJFU124+3h3H+buw/r37x/2Y0Wyilt7/rjlV+I3bHeYgLAQGGJmg82sHzAKmBJm\n5UH6Z4BH3X1S2rwzg/8GXArE6wkOia24t+bX8wjRF9en4bMGBHdvA24ApgFrgInuvsrMxpjZJQBm\n9lEzawAuB+4zs1XB4v8AXAB8LUPz0sfNbAWwAjgDuK2oWyYiIjkJ08oId58KTE2bdnPK64UkipLS\nl3sMeKybdX46p5yKiEhJhQoIItXiK799g9c37Kl0NkQiSV1XSK9SDcFAlctSKgoIIjGhymQpNQWE\nHtzwxGLunbMhe0IRkSqggNCD55Zv584X11Y6GyISU3EbrlUBQXotFcFIqcT1yFJAkF5LlbNSKnE9\nshQQRERKJG5PLCsgiIgIoIAgIiIBBQQREQEUEEREJKCAICJSInoOQXqlZVv2891Jy+joiNkvQHKy\ndMt+PG5nuQqIWeOi4xQQpCi+/vBCJi1qYN+RlkpnpepV6nw8ffVOLh03lycXbMmeWGJJAUEkJird\npv3NPYcBqNt1qLIZkZIJFRDMbISZ1ZpZnZmNzjD/AjNbbGZtZnZZ2ryrzWx98Hd1yvTzzGxFsM57\ngqE0RUSkQrIGBDPrA4wDLgaGAleY2dC0ZJuBrwFPpC17GnAL8DFgOHCLmZ0azL4XuBYYEvyNyHsr\nRPKgonCRzsLcIQwH6tx9o7u3ABOAkakJ3L3e3ZcDHWnLfg6Y7u573X0fMB0YYWZnAie7+zxP1FA9\nClxa6MaIiEj+wgSEAUBqLVJDMC2M7pYdELzOZ50iIlICYQJCprL9sDfb3S0bep1mdq2Z1ZhZTWNj\nY8iPLa7W9vQbHxGRMOJVLhkmIDQAZ6W8HwhsC7n+7pZtCF5nXae7j3f3Ye4+rH///iE/trgONbdV\n5HPjKF6Hf7xEpc5D3YZnF9c2MmECwkJgiJkNNrN+wChgSsj1TwMuMrNTg8rki4Bp7r4dOGhm5wet\ni64Cns0j/xIR8Tz846lS55q4nuQkvKwBwd3bgBtInNzXABPdfZWZjTGzSwDM7KNm1gBcDtxnZquC\nZfcCt5IIKguBMcE0gOuB+4E6YAPwQlG3TEREctI3TCJ3nwpMTZt2c8rrhXQuAkpN9yDwYIbpNcCH\ncsmsSDHpglekMz2pLCJSZHHt70kBIYR47loRqbx43YYqIEhRxDFoxvQiLrb5zmTVtiZe37C70tmQ\nQKg6BBGpvGqs8/jCPa8BUH/HFyqcEwHdIUiRVOG5SqQI4nU7p4DQCxxsbq10FkR6lbg+s6GA0Atc\n8qu5odPuaGrWqGcivZQCQi+waffhUOnqdx/m/J/M5N6XN5Q4RyISRQoI3Zhb1/taPmzdfxTondsu\nIgoIGa1oaOLK++dXOhsikVRNzV6lMwWEDPZqoPi86WRRveJZTSq5UECQoohpowoRSaGAIL2W7mak\n1OJ2jCkgiIgUWVxvmBUQQohrz4VSPW57bjX3zlFz4EJt23+UpqN6ULM7CggZxDW6V5JiZmnd/9qm\nSmehKvzVHbO48K6XK52NyAoVEMxshJnVmlmdmY3OMP8kM3sqmD/fzAYF0680s6Upfx1mdk4wb06w\nzuS89xRzw6QyVLksUdd48FilsxBZWQOCmfUBxgEXA0OBK8xsaFqybwD73P0DwFjgTgB3f9zdz3H3\nc4CvAvXuvjRluSuT8919VxG2RypMdwoi8RXmDmE4UOfuG929BZgAjExLMxJ4JHg9GfiMde3d6Qrg\nyUIyK6VVyMk8jncGccyzSCmFCQgDgC0p7xuCaRnTuHsb0AScnpbmy3QNCA8FxUU/zBBApEK0J0QK\nE9cb5TABIdPpIX17e0xjZh8Djrj7ypT5V7r72cAng7+vZvxws2vNrMbMahobG0NkVwqlYh+R3ilM\nQGgAzkp5PxDY1l0aM+sLnALsTZk/irS7A3ffGvw/CDxBomiqC3cf7+7D3H1Y//79Q2S3cnYeaOYn\nL6yJbffRujOQuNq85wit7R2VzsZxcf0phQkIC4EhZjbYzPqROLlPSUszBbg6eH0ZMMuDxvtm9hbg\nchJ1DwTT+prZGcHrtwJfBFYSc/8+cRn3vbyRhfV7syeOiXGz63hx5Y5KZ0OkW40Hj3HBz2Zz63Or\nK52V2MsaEII6gRuAacAaYKK7rzKzMWZ2SZDsAeB0M6sDbgRSm6ZeADS4+8aUaScB08xsObAU2Ar8\ntuCtqbCW4AolnvcHmf1sWi3XPbao0tkoiZ6KxhbW72Xc7LrQ65q+emfocSfiKqp3kE1HE51Rqtv2\nwvUNk8jdpwJT06bdnPK6mcRdQKZl5wDnp007DJyXY14lBrxKwuHlv5kHwL986gOh0v/zozWABouX\neNOTylIkEb18TPG7efWVzoJIpCkgZJB+a1wd17zygLp/EOmRAoKI5K25tT0yjQ7ieuH24f96iW9P\nWFLpbAAKCFI0cf05SiHueGEt1z22qMIt66JbXLnzYDMrtzb1mKbpaCvPLk1vyV8ZCghF8uBrm1iw\nKd7NTYvxQJpF+McZNweaWzn/9pksenNfpbPSrS17jwDQdERdSqdKFjuv3HqAL/7ytcpmJgcKCEXy\n3y/VVjoLRRPV5oWFilvvKEs272fHgWbunrGu0lmRXkIBIQNd5eavWpqdivRGCgjSRX5FRwqivZku\nA6qDAkIG1XiVG+ax/mKVqLR3OF9/aEHk61Sqby+XX8xK4SQLBYQiiXr5dGob/AWb9vKLGetL9lmN\nB48xu7aR//fk4pJ9hvQedbsO8uae6u4WJCoUEEJoaetg6/6jlc5G0fzDffMYq4pKyVNza3tZP+/C\nu17hb342p6yf2VspIITwrxOW8ok7ZtEWoe51Rcpt1bYDAExYeGK8rEiNnRGlvIQUpS67QQEho/RW\nRguCh27aI3X0R5++rurSdDSazxpEvLS2R88s2VrpLHSigCAFW7BpL7sPHUu88ej+QNOzFdFsxkpU\n93Vc6A5Bqs7UFdsr+vkPz92U09gFkh+d+6tfqPEQJCFsEUhcfzhxLeL50R8STWrDjl0gIpmFukMw\nsxFmVmtmdWY2OsP8k8zsqWD+fDMbFEwfZGZHzWxp8PeblGXOM7MVwTL3WNTbbfYixdoTMY0vkgeP\n69VEhUWtV4SsAcHM+gDjgIuBocAVZjY0Ldk3gH3u/gFgLHBnyrwN7n5O8HddyvR7gWuBIcHfiPw3\no7jyOSGmLtKrfxrROr47i3LeclCpk2/m30VpvtSmI60cOtZWknVL98LcIQwH6tx9o7u3ABOAkWlp\nRgKPBK8nA5/p6YrfzM4ETnb3eZ44uh8FLs059xIJ3e3pyJ1/vce3EiEfHvMS5946vdLZ6HXCBIQB\nwJaU9w3BtIxp3L0NaAJOD+YNNrMlZvaymX0yJX1DlnVKhejuX8Ir3cHS0lb5Fjj3zFzPjNU7K52N\nsgkTEDJd6KUfBd2l2Q78ibt/BLgReMLMTg65zsSKza41sxozq2lsbAyRXclXUeoOPONLKZNP3DGL\nHz+fvd+qYotCFWApjre7pq/jmkdrSrDmaAoTEBqAs1LeDwTSh/c5nsbM+gKnAHvd/Zi77wFw90XA\nBuB/B+kHZlknwXLj3X2Yuw/r379/iOxKpVX+1NCNyGYsNz2dfLfuP8pvXy3/2NGVrFSO4m6NQHzM\nS5iAsBAYYmaDzawfMAqYkpZmCnB18PoyYJa7u5n1DyqlMbP3kag83uju24GDZnZ+UNdwFfBsEbZH\nREJoPHiMQaOfZ96GPaGXydwiJqZnPskoa0AI6gRuAKYBa4CJ7r7KzMaY2SVBsgeA082sjkTRULJp\n6gXAcjNbRqKy+Tp3T/aJfD1wP1BH4s7hhSJtk4gAizd3P/RmcljOh+aGv5uI61WvhBfqwTR3nwpM\nTZt2c8rrZuDyDMv9Hvh9N+usAT6US2bjQr+bBFVOV9bf/fr1oq5PAaH4ovadquuKHKSe4I61tbP/\nSEvlMhMhXYoSInaQJ0U0W1VBsb86KCDk6ZpHajhnTEo76So42xTtij5Yz+5Dx5i8qKHntGWkk1Zh\nMtUhRO0KNy6aW9v51az16tyuWry6fnels1AyhfzI00+635m0rKC8SPltbDzE7VPXhGo5VOpiweO9\n6FaZ37y8gZ+/tI4n5m+udFY6UUDIoNCLnnx+I/uPtLDrQHPOy+1oao5ePzIxuWqM3PdWAZm+gWse\nqWH8KxvZvPdI6PWUapcPu21GidZcWUdbEqPOHS3z6HPZKCBExHm3zWD47TNzWmbN9gOc/5OZPDrv\nzRLlqrrEJE5lVb/7MHfPWFeygNbWkft6oxBaoxTgs3ZaF8yOUJYBBYTIaM/jR1i/OzHweC5tyaMu\n2T5+Yf3e7Il7qUmLGrh7xnp2Hii8OKXQIBmFOoQoPCWdq2TA8EiE0hMUEKRgqb/HQn+aCzYlAkEu\n7eOlTOJ33q2YbCf6qMYwBYQqUOyrjEJvY6PWx3t34nhlmSrm2e/VorrrFBByEPbEW66dXewTQpRO\nMKUoW417ACi3qJVvV5PkodgRrVanCgjFolNNQqHnkEqdszftPsyg0c+zpIfuHqIml6/qkz+dBSTq\nqjpS6qty2V89fZ6CR26O1yFE7ItTQJCyaGnr4IHXKlsv0NOP7+XaXQD8z5Kt5cpO4XKICFv2HgXg\n/f8xlS/96rVQgTdUmvBZkBRRvVkN1ZeRRFvELjIyuu/lDfz39HWVzoYAq7YdyOuYyVTkFoNDL5KS\n32TUvj/dIchxpQwsB/MYH/f1ut1MW7WjBLnpXtR+oD0pRuV9sS5Uo3rFWynZn0OI5hemO4RMCtxX\n5TuplOagyvVYLUUu3OEr988HoP6OLxRlnT1VKqvCuatcLhCicJcagSyEljzatjfl3jtBKekOQcqi\np9Nt6kN55Twt//OjNWxvOlrGTyyuZAxzd15cuSOvhxtz+ryQ08otCnnIVVSvPxQQclCOq6C1Ow6U\n/kNKLNeD/f3/MTV7ohJ5enGMKpHTJL/m55Zv57rHFnH/qxtzXkdPh3RUT1rVIExxX9ORVq55ZGFZ\nO/gLFRDMbISZ1ZpZnZmNzjD/JDN7Kpg/38wGBdM/a2aLzGxF8P/TKcvMCda5NPh7T7E2qtQ6Sngl\nNuLuV3NeJkq3ylEoOsjVQ3M3ccuUVUA88994MHHCyKX4IZ+TvQJEeT2xYDMz1uzit3kE+nxlrUMI\nxkQeB3wWaAAWmtkUd1+dkuwbwD53/4CZjQLuBL4M7Aa+5O7bzOxDJIbhHJCy3JXByGmx0p7hrJFa\nBh3XB9OqXXdf163Pre5mTrQVUu9RrMD3ekz70TrY3Mo7+vWlz1v0I0oV5g5hOFDn7hvdvQWYAIxM\nSzMSeCR4PRn4jJmZuy9x923B9FXA28zspGJkvBJiePEYHTn+7orRHcczSxo6FaN0t8bUG76GfeG7\nfK4GhTxsVrfrIIeOtx6L16/j7B+9xC1TVlY6G5ETJiAMALakvG+g81V+pzTu3gY0Aaenpfl7YIm7\npxaIPRQUF/3QItTMI1v5XmQyWiKFXD12uxdDrrOYR8G/PbWM255fk9Mys2sbi5eBEivVcRh2HzQd\nzb0pMcD3Ji/jc2Nf6TStvcPZksP4C8XwTIzrj0olTEDIdHik/7x7TGNmf0GiGOn/psy/0t3PBj4Z\n/H0144ebXWtmNWZW09hYnh9r1Lqk7U6xTwj5noyjE8p7l56O0k/cMSv/9eZ1+Ic/CCbWNFC782Cn\naT9/qZZP/nR2WYPC4Zb2ktYHxlGYgNAAnJXyfiCwrbs0ZtYXOAXYG7wfCDwDXOXuG5ILuPvW4P9B\n4AkSRVNduPt4dx/m7sP69+8fZpt6nVx+wJuCMRTKLuT5opS/z94Ut7buD9ecNpdWRunfXzEvBJ5f\nvh0o/5CZr9WVaCjcmB5sYQLCQmCImQ02s37AKGBKWpopwNXB68uAWe7uZvZu4HngJnefm0xsZn3N\n7Izg9VuBLwKRL9CLWkdUuVqwaS+f+vmcSmejR998fDEQz9Y+5VbI8Zh6Mn926dYinIgL22HJ4ToL\nWUs+X0epn92Im6wBIagTuIFEC6E1wER3X2VmY8zskiDZA8DpZlYH3Agkm6beAHwA+GFa89KTgGlm\nthxYCmwFflvMDSuluB5CGxoP9Ti/GCdh9/JcHCV7J521dmfR1/3TF9cWfZ1RtfvQMb49YSnXPNK1\nsV/68RCmmu/ldY3Hm8GWS9yKLI+1tdMR0SueUF1XuPtUYGratJtTXjcDl2dY7jbgtm5We174bJZX\noX3ERHNXhz9Rx+EHluym+g/Lthd93b+esyF7oggoxtV0a3uiQ/6dB048wxB2/6cnc3eufnAB7+//\nTmb++9/mna9xs+vyWjbdqm1NtLY755z17qKsr1j+7D9frHQWuqW+jIqkEifRCDXMyioKo6jF6Ovq\n5NX13Zdzt3c4S7bsz3vdueyXsCk3NOZfT7VyaxM/m1ab9/KpvnDPa0DPfWGVrAFJVK8Ks1DXFTkI\nu49veXZVSfPRVbSOvjCB6tmlavJXDPfMXM8flqW38ShMt6UZWXZrMUpB2kpQpn/l/W/w7xOXFX29\n1UgBIQ/ZDvz0JnVRUaor5PQAkKmyM/2zvz1haY/rLEWIy7Tf1myPb99R7lC7o7Bjracr5GzHS1zu\nUOfW7eH3ixsqnY1YUEDIQ1yeU4iSiNah8dzy4tdBlFO+XS9E9Vxe7myV67i88aml/J9fz82eMMW+\nIy0lyk33emVAWPTmPiYs2Nzt/O5+LMmD54UV5R20RYqjWCfB+t2H+btfz6XpaGtxVpgnx0t6Ys/1\nZFmMc2tErxtyl7Zfnl6ylSWbw9f1tLR1MP6V8nVql9QrA8Lf3/s6o59ekffyYR76eXRePXfPODFk\npLsfb9FRLFG8yEucpLrmLKpXpPn4xcz1LN68nxmri9/kNVeFds6W6aSfbyujOIrqneuxtvaKfK5a\nGRXI3VnW0NRl+s1BxXKHw7c/M4Qxf1jFI/PeZNNPPl/0steoHtRRMqd2F/V7itMtQpTGw31LnsdS\n8phZtS18HUpPLZKKdQwW8stQUW7heuUdQja5HNwPzq3n0nFz2X8kc/HBPTPXM331Dh6Z92bO6y62\nsE0Mc83j/E17c/6MYuchm689tJCWtiLdofWwib+Ysb4odw4Hmlu54YnFPSfy0tx55fvdF+NJ/jBr\nONbWzvqUhhtRaNJcbJU6TSgg5CDZ1W/qjzBMX/qt7SfKeit5DfO93y8v+jrn1O5iWUo7+Lum17Jy\na9c7pnLavOcIzywpfquSIy1tHGk50cNnphPg2BnruObRwof4+N28NznY3HNvotuampm/cW+PaQqR\nPGb3HW7hR1NW0dZR3CLPfP3gmZV8duwrRen3SPcUnanIKAefuGNWwQO+J04i4a9oRtz9Cu/o14en\nv/mJgj43F7lcde5IG6VrYk0DE2s6n4wnL2qgubVrmeiKhiZ+8kJu3VOH8YVfvpr1ZJqPoTdPA+Dv\nzx1Y9HWnC7MPLh3XudXKnNpdode/eW/2h8eS8e6259fw+8UN9Otz4vrxQHNrpzxe//hiXh/9aQoV\n5tBbENyRHj7WxhnvKt/wKm3tHazadoAPB08+7zzQTN+3GKeXIA+VKknQHUIGJW25kWP6tTsOsrib\n1glRqKgNk4fvTFrGQ3Pru0y/6ZnlJRlxqxTBIJNc9uXKrU1szqEOI59ikFzqSG6f2rm/psPH2o7n\nL32ftgd3Bi0pjSJaMxS/jbj7lS7TAAaNfp6vPjA/VL7KfR7MpZjrrunrGDlu7vE74I/dPpPzbptR\nqqxVhAJCmRyviCzBER/X295il/2+uac8XXvnE4i/+MvXuOBns0v6GYU40NzWbf7CNoI40EMgztT9\nRql6D561difDf9z1RF3oWAvNRsftAAAMRElEQVTJCvhydN6Xy8VDMfW6gLDrQPiByIvlWFvH8R9V\nmF4Ot+0/yqDRz2cti0/+TrftP5p3mfn9r24M3Xd+xjwUcFJf0eP25X6y+Jufzck4fUqRu3Y4zhPt\nxaPepXi+Hn69nubW9ox7+K/vnM0lv8rtQat0mbqpyHY0tbZ3ZO2yeswfVrMrw0k7051jLkfZiXrA\n7EsVGs9fWV+Zkft6XUD41oQlZf/M70xallO/6zPXJsqCn+jh4Tk4cbexdsdB/u2pZTm3Xd7edJTb\nnl/DPz20sNP0uXV7ur16O9rS3rm1TomuZHu62szVt54s7j5P3eRdB5szDjo0qWZLQVeSlSwNTO76\nh1+v77bn0aMZ6oTKYcgPXjh+AdNd0WB3dzRvKfBsV459ctf0dQwa/XwZPimzXhcQjrYW3lKikKvi\nMHfJmYqXLhr7ctZb7FzvwJMx6mBz1yaz+460dnqwLumDN7/IF3/5am4flIcFm0rXeiYfmR4q7O4h\nPIDvTl7OqPHz8v68KNQPARw42hqZJ9DS6yiuf3xRxnTdZbdPDl/qlr1H+Mf7O9d7JPd1ISVd2X7D\n98xcn//Ki6DXBYQwSnn8p95uTl+9k5Hj5nYZ1/XEcXti+rqdh7oML5l+fOd6oH7hnu5P7NNX7+Du\nGScOzn2HW3g9GG5w3c5DfOrnczjY3FrWc8Wbew7zlz+aVpHy1esfO3HySf3ee3pQuGLDlRagpa3j\n+OhlSaVq55/peO3pEF6b1pHflr2dizqPr6+b7GYK3t39ZsZOX9dleM2e6gHrdh3ixZXZ+8X6yK3T\nu52373D5+y5KFyogmNkIM6s1szozG51h/klm9lQwf76ZDUqZd1MwvdbMPhd2nZWU7eq0kKu3SSlN\nMr/15BKWbdnf5fY7+QNMb/ad9Q4h+Dm1hewiI/kwXbL1SGvKB37/95279vjIrdP5SsoV06bdh1m8\neX/ZerxsOtrKrc+t4UBzG08vaeDGMnVnvG7nQQ42tzJjTdcmnU1HW3l6cW7deO9P67DsxZXb2XWw\na71WpR62un1q12bA+e7i+Rv38PK67svCi/VkcTJ/DfuOsutgc7dPb9fnEqB72GYHfpL2PV1418tc\n91iWBwmh2wdYoedgUS5ZA4KZ9QHGARcDQ4ErzGxoWrJvAPvc/QPAWODOYNmhJMZg/gtgBPBrM+sT\ncp1Ft3nPkU4PUR061pax7fZ/T+9aVJKqkJ/qj5/P3u4+OVZA+g8m9Q7h2aVb2bT7SMb5kxblVsG8\n+1AL598+k5r63Ipp5tTuKttwmR/+r5eYsSbxBHDyfybFHiP3orGv8I8PLOg0Ldmp3e1T12YdzOXw\nsc4Ps33+Fyfuyppb27nuscVdiiagNEVGU1dkv4L93RtvFuWz3J0vj3+Dqx888d09PHdTp/qcTNc3\nExduyelzfvg/nYdiH/7jmdTtyjxUbE8PDH7pl69x/6sb+Y9nVrD70LGMATm5T1rbO7ivm47nBo1+\nnlXbCn8wM7WebvmWJtaVqUv9MA+mDQfq3H0jgJlNAEYCqY/ojgR+FLyeDPzKEpeOI4EJ7n4M2BSM\nuTw8SJdtnUXzwortXP941+j982m1PPx6PQAvf/dv+e2rGxl5zoAe11VohU978Cs42tJ+/M4gvdfM\nZFcQ6T+YDY2HWLm1iS99+I8zjifw9OIGrvr4II605F7ht+NAM+Nm5zZ05ENz6/neiD/L+bNyld6K\nZ+XWE/3vPLOk8xV6Kcpgl6WNSDZtVbiuKToc/uKWaZ2mbWtq5pklDXz+7DM5HDz5niz6qKnfy2W/\nmcer3/tUEXLd1Tcz/AbSpQfUZJcruTo3w9Xuj/6Q/eed61giv3vjTa694H05LZPqupSiwGSrtyfm\nb+aLf3lmp3TfnbTs+F1itpaCv5pVx5c/elbeeYJEB4pJ8zbu4aKxrxT8UGwYlq0YwswuA0a4+zXB\n+68CH3P3G1LSrAzSNATvNwAfIxEk3nD3x4LpDwAvBIv1uM5Mhg0b5jU1uXcLUMla+3L709PfwZsV\nasMs+XtrH6O1Pa5PlOTnff3fycYChttMOuu0t3epTyinIe95F+u7uSspple/9ynOOu0deS1rZovc\nfVi2dGHqEDLdvKYfud2lyXV61w83u9bMasysprExv7a55/5J9kG2PzzwlLzWnauzB5zC4DPe2Wla\nv76Zd8Mnh5zRqbuAPzqp8w3de/6o6yPzQ888mQs/+N4i5DQe3nty+bouKKVT3t7v+OsLP/jeXrEP\nP/i/Ti7Kes4eUJ7fbneGvPddDDo9vxN1LspRXRemyKgBSL3/GQikP+mTTNNgZn2BU4C9WZbNtk4A\n3H08MB4Sdwgh8ttFOfsBEpHwxlU6A9JJmDuEhcAQMxtsZv1IVBJPSUszBbg6eH0ZMMsTZVFTgFFB\nK6TBwBBgQch1iohIGWW9Q3D3NjO7AZgG9AEedPdVZjYGqHH3KcADwO+CSuO9JE7wBOkmkqgsbgP+\nxd3bATKts/ibJyIiYWWtVI6SfCuVRUR6s2JWKouISC+ggCAiIoACgoiIBBQQREQEUEAQEZFArFoZ\nmVkjkG/vW2cAXcfxq27a5t5B21z9Ct3eP3X3/tkSxSogFMLMasI0u6om2ubeQdtc/cq1vSoyEhER\nQAFBREQCvSkgjK90BipA29w7aJurX1m2t9fUIYiISM960x2CiIj0oFcEBDMbYWa1ZlZnZqMrnZ98\nmdlZZjbbzNaY2Soz+3Yw/TQzm25m64P/pwbTzczuCbZ7uZmdm7Kuq4P0683s6u4+MyqCsbiXmNlz\nwfvBZjY/yP9TQTfqBF2tPxVs83wzG5SyjpuC6bVm9rnKbEk4ZvZuM5tsZmuD/f3xat/PZvZvwXG9\n0syeNLO3Vdt+NrMHzWxXMMpkclrR9quZnWdmK4Jl7jHLcVgdd6/qPxLda28A3gf0A5YBQyudrzy3\n5Uzg3OD1HwHrgKHAT4HRwfTRwJ3B68+TGLLUgPOB+cH004CNwf9Tg9enVnr7smz7jcATwHPB+4nA\nqOD1b4Drg9ffBH4TvB4FPBW8Hhrs+5OAwcEx0afS29XD9j4CXBO87ge8u5r3MzAA2AS8PWX/fq3a\n9jNwAXAusDJlWtH2K4nxZj4eLPMCcHFO+av0F1SGHfBxYFrK+5uAmyqdryJt27PAZ4Fa4Mxg2plA\nbfD6PuCKlPS1wfwrgPtSpndKF7U/EiPqzQQ+DTwXHOy7gb7p+5jEGBsfD173DdJZ+n5PTRe1P+Dk\n4ORoadOrdj8HAWFLcJLrG+znz1XjfgYGpQWEouzXYN7alOmd0oX56w1FRskDLakhmBZrwS3yR4D5\nwHvdfTtA8P89QbLutj1u38ndwPeAjuD96cB+d28L3qfm//i2BfObgvRx2ub3AY3AQ0Ex2f1m9k6q\neD+7+1bg58BmYDuJ/baI6t7PScXarwOC1+nTQ+sNASFTGVqsm1aZ2buA3wP/6u4HekqaYZr3MD1y\nzOyLwC53X5Q6OUNSzzIvNttM4or3XOBed/8IcJhEUUJ3Yr/NQbn5SBLFPH8MvBO4OEPSatrP2eS6\njQVve28ICA3AWSnvBwLbKpSXgpnZW0kEg8fd/elg8k4zOzOYfyawK5je3bbH6Tv5BHCJmdUDE0gU\nG90NvNvMkkPApub/+LYF808hMaxrnLa5AWhw9/nB+8kkAkQ17+cLgU3u3ujurcDTwF9R3fs5qVj7\ntSF4nT49tN4QEBYCQ4LWCv1IVEBNqXCe8hK0GHgAWOPud6XMmgIkWxpcTaJuITn9qqC1wvlAU3BL\nOg24yMxODa7MLgqmRY673+TuA919EIl9N8vdrwRmA5cFydK3OfldXBak92D6qKB1ymBgCIkKuMhx\n9x3AFjP7s2DSZ0iMS161+5lEUdH5ZvaO4DhPbnPV7ucURdmvwbyDZnZ+8B1elbKucCpdwVKmSpzP\nk2iRswH4QaXzU8B2/DWJW8DlwNLg7/Mkyk5nAuuD/6cF6Q0YF2z3CmBYyrr+CagL/r5e6W0Luf1/\ny4lWRu8j8UOvAyYBJwXT3xa8rwvmvy9l+R8E30UtOba+qMC2ngPUBPv6f0i0Jqnq/Qz8F7AWWAn8\njkRLoaraz8CTJOpIWklc0X+jmPsVGBZ8fxuAX5HWMCHbn55UFhERoHcUGYmISAgKCCIiAiggiIhI\nQAFBREQABQQREQkoIIiICKCAICIiAQUEEREB4P8DgHVERNWpnD0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[190000:200000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More sanity checks\n",
    "\n",
    "1. Accuracy should be a lot higher than 0.5 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99991"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.eval_accuracy(net1, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Histogram should reveal two blobs, not one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Model outputs after training')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram_results = utils.histogram_outputs(net1, 500)\n",
    "plt.plot(histogram_results[1], histogram_results[0])\n",
    "plt.xlabel('Model output')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Model outputs after training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error rates on both spheres (estimated from 100000 points):\n",
    "\n",
    "* After 717766 training steps - 9e-5\n",
    "* After 500 training steps - 3e-3\n",
    "* After 1000 training steps - 3e-3\n",
    "* After 2000 training steps - 1.7e-3\n",
    "* After 3000 training steps - 1.3e-3\n",
    "* After 4000 training steps - 5.9e-4\n",
    "* After 5000 training steps - 6.4e-4\n",
    "* After 10000 training steps - 5.8e-4\n",
    "* After 100000 training steps - 2.7e-4\n",
    "\n",
    "Error rates on the inner sphere only (estimated from 1 million points):\n",
    "* After 10 training steps - 1.7e-4\n",
    "* After 500 training steps - 1.5e-4\n",
    "* After 1000 training steps - 1.6e-4\n",
    "* After 2000 training steps - 1.6e-4\n",
    "* After 3000 training steps - 1.7e-4\n",
    "* After 4000 training steps - 1.6e-4\n",
    "* After 5000 training steps - 1.3e-4\n",
    "* After 10000 training steps - 1.4e-4\n",
    "* After 100000 training steps - 1.6e-4\n",
    "* After 717766 training steps - 1.5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_load = checkpoint_steps + [717766]\n",
    "\n",
    "model_errs = {}\n",
    "\n",
    "for model_num in models_to_load:\n",
    "\n",
    "    test_net = model.LargeReLU(sphere_dim=500,\n",
    "                     n_hidden=1000)\n",
    "\n",
    "    loaded_parms = torch.load('trained_models/largeReLU_{}_{}.pth'.format(1, model_to_load))\n",
    "    test_net.load_state_dict(loaded_parms['model_state_dict'])\n",
    "    \n",
    "    model_errs[model_num] = 1 - utils.eval_accuracy_single(test_net, 1000000, radius=1, desired_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attacking the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = 500\n",
    "\n",
    "test_net = model.LargeReLU(sphere_dim=500,\n",
    "                 n_hidden=1000)\n",
    "\n",
    "loaded_parms = torch.load('trained_models/largeReLU_{}_{}.pth'.format(1, model_to_load))\n",
    "test_net.load_state_dict(loaded_parms['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dataset.single_sphere_dataset()\n",
    "\n",
    "test_idx = 0\n",
    "test_point = dset[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, loss 0.0009934734553098679\n",
      "Iter 100, loss 0.0011121999705210328\n",
      "Iter 200, loss 0.0012761552352458239\n",
      "Iter 300, loss 0.0015628041001036763\n",
      "Iter 400, loss 0.00226788641884923\n",
      "Iter 500, loss 0.015628362074494362\n"
     ]
    }
   ],
   "source": [
    "perturbed_point = attacks.projected_GD(test_net, test_point, 0, max_iter=10000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.0343], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net(perturbed_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, loss 0.0002809368306770921\n",
      "Iter 100, loss 0.0002862997353076935\n",
      "Iter 200, loss 0.0002917817619163543\n",
      "Iter 300, loss 0.00029762129997834563\n",
      "Iter 400, loss 0.000303818320389837\n",
      "Iter 500, loss 0.0003104920033365488\n",
      "Iter 600, loss 0.00031740395934320986\n",
      "Iter 700, loss 0.000324673397699371\n",
      "Iter 800, loss 0.0003325386205688119\n",
      "Iter 900, loss 0.0003408804477658123\n",
      "Iter 1000, loss 0.00034981805947609246\n",
      "Iter 1100, loss 0.0003594706067815423\n",
      "Iter 1200, loss 0.00036971885128878057\n",
      "Iter 1300, loss 0.00038092033355496824\n",
      "Iter 1400, loss 0.00039307496626861393\n",
      "Iter 1500, loss 0.000406301929615438\n",
      "Iter 1600, loss 0.00042083943844772875\n",
      "Iter 1700, loss 0.0004368066438473761\n",
      "Iter 1800, loss 0.00045468006283044815\n",
      "Iter 1900, loss 0.00047469791024923325\n",
      "Iter 2000, loss 0.0004970983718521893\n",
      "Iter 2100, loss 0.0005222387262620032\n",
      "Iter 2200, loss 0.0005504761938937008\n",
      "Iter 2300, loss 0.0005822870298288763\n",
      "Iter 2400, loss 0.0006188622792251408\n",
      "Iter 2500, loss 0.0006613928126171231\n",
      "Iter 2600, loss 0.0007116645574569702\n",
      "Iter 2700, loss 0.0007717015105299652\n",
      "Iter 2800, loss 0.0008437649230472744\n",
      "Iter 2900, loss 0.0009334497735835612\n",
      "Iter 3000, loss 0.0010513499146327376\n",
      "Iter 3100, loss 0.0012187680695205927\n",
      "Iter 3200, loss 0.0014804366510361433\n",
      "Iter 3300, loss 0.0021214615553617477\n",
      "Iter 3400, loss 0.006676272489130497\n",
      "Iter 0, loss 0.0015253110323101282\n",
      "Iter 100, loss 0.002302497159689665\n",
      "Iter 0, loss 0.0003203832311555743\n",
      "Iter 100, loss 0.00032824851223267615\n",
      "Iter 200, loss 0.0003367095487192273\n",
      "Iter 300, loss 0.0003457663697190583\n",
      "Iter 400, loss 0.00035565727739594877\n",
      "Iter 500, loss 0.00036614391137845814\n",
      "Iter 600, loss 0.00037746457383036613\n",
      "Iter 700, loss 0.0003895000845659524\n",
      "Iter 800, loss 0.0004027270770166069\n",
      "Iter 900, loss 0.0004175029753241688\n",
      "Iter 1000, loss 0.0004334702098276466\n",
      "Iter 1100, loss 0.00045098623377270997\n",
      "Iter 1200, loss 0.00047064671525731683\n",
      "Iter 1300, loss 0.0004925706889480352\n",
      "Iter 1400, loss 0.0005168771021999419\n",
      "Iter 1500, loss 0.0005442806868813932\n",
      "Iter 1600, loss 0.0005758534534834325\n",
      "Iter 1700, loss 0.0006127863889560103\n",
      "Iter 1800, loss 0.0006563892820850015\n",
      "Iter 1900, loss 0.0007083290838636458\n",
      "Iter 2000, loss 0.0007712250226177275\n",
      "Iter 2100, loss 0.0008482910343445837\n",
      "Iter 2200, loss 0.0009482178138568997\n",
      "Iter 2300, loss 0.001085764612071216\n",
      "Iter 2400, loss 0.0012890134239569306\n",
      "Iter 2500, loss 0.001699790358543396\n",
      "Iter 2600, loss 0.0030958366114646196\n",
      "Iter 0, loss 0.0023858672939240932\n",
      "Iter 0, loss 0.001073261140845716\n",
      "Iter 100, loss 0.0012617491884157062\n",
      "Iter 200, loss 0.0016351675149053335\n",
      "Iter 300, loss 0.002711193636059761\n",
      "Iter 0, loss 0.0014068715972825885\n",
      "Iter 100, loss 0.0020592452492564917\n",
      "Iter 200, loss 0.01263741496950388\n",
      "Iter 0, loss 0.00030071981018409133\n",
      "Iter 100, loss 0.0003054867556784302\n",
      "Iter 200, loss 0.0003104920033365488\n",
      "Iter 300, loss 0.0003156163729727268\n",
      "Iter 400, loss 0.0003209791029803455\n",
      "Iter 500, loss 0.00032646095496602356\n",
      "Iter 600, loss 0.0003323002893012017\n",
      "Iter 700, loss 0.00033849707688204944\n",
      "Iter 800, loss 0.00034505134681239724\n",
      "Iter 900, loss 0.0003518439189065248\n",
      "Iter 1000, loss 0.0003591130953282118\n",
      "Iter 1100, loss 0.0003670972364488989\n",
      "Iter 1200, loss 0.00037531962152570486\n",
      "Iter 1300, loss 0.00038425691309385\n",
      "Iter 1400, loss 0.0003936707798857242\n",
      "Iter 1500, loss 0.0004037995240651071\n",
      "Iter 1600, loss 0.00041476229671388865\n",
      "Iter 1700, loss 0.0004266782198101282\n",
      "Iter 1800, loss 0.0004396664153318852\n",
      "Iter 1900, loss 0.0004536076739896089\n",
      "Iter 2000, loss 0.0004686211177613586\n",
      "Iter 2100, loss 0.00048494499060325325\n",
      "Iter 2200, loss 0.000502817565575242\n",
      "Iter 2300, loss 0.000522357877343893\n",
      "Iter 2400, loss 0.0005440423847176135\n",
      "Iter 2500, loss 0.0005683475756086409\n",
      "Iter 2600, loss 0.0005955114611424506\n",
      "Iter 2700, loss 0.0006260104128159583\n",
      "Iter 2800, loss 0.000660439720377326\n",
      "Iter 2900, loss 0.0006997520686127245\n",
      "Iter 3000, loss 0.0007448997348546982\n",
      "Iter 3100, loss 0.0007969540893100202\n",
      "Iter 3200, loss 0.0008579387213103473\n",
      "Iter 3300, loss 0.000931663322262466\n",
      "Iter 3400, loss 0.0010222929995507002\n",
      "Iter 3500, loss 0.0011392300948500633\n",
      "Iter 3600, loss 0.0012965138303115964\n",
      "Iter 3700, loss 0.001542688929475844\n",
      "Iter 3800, loss 0.002092197770252824\n",
      "Iter 3900, loss 0.003912296146154404\n",
      "Iter 0, loss 0.0018173621501773596\n",
      "Iter 100, loss 0.0038807096425443888\n",
      "Iter 0, loss 0.0004303721070755273\n",
      "Iter 100, loss 0.0004451475979294628\n",
      "Iter 200, loss 0.00046135272714309394\n",
      "Iter 300, loss 0.00047922570956870914\n",
      "Iter 400, loss 0.0004988856380805373\n",
      "Iter 500, loss 0.0005208089714869857\n",
      "Iter 600, loss 0.0005452338373288512\n",
      "Iter 700, loss 0.0005727558163926005\n",
      "Iter 800, loss 0.0006043276516720653\n",
      "Iter 900, loss 0.0006406639004126191\n",
      "Iter 1000, loss 0.0006825978052802384\n",
      "Iter 1100, loss 0.0007314390386454761\n",
      "Iter 1200, loss 0.0007893307483755052\n",
      "Iter 1300, loss 0.000860320869833231\n",
      "Iter 1400, loss 0.0009556017466820776\n",
      "Iter 1500, loss 0.00108624086715281\n",
      "Iter 1600, loss 0.0012729407753795385\n",
      "Iter 1700, loss 0.00162743148393929\n",
      "Iter 1800, loss 0.0027018017135560513\n",
      "Iter 0, loss 0.000428108120104298\n",
      "Iter 100, loss 0.00044324109330773354\n",
      "Iter 200, loss 0.000459565402707085\n",
      "Iter 300, loss 0.00047803416964598\n",
      "Iter 400, loss 0.0004984090337529778\n",
      "Iter 500, loss 0.0005210472736507654\n",
      "Iter 600, loss 0.0005460678366944194\n",
      "Iter 700, loss 0.0005740663618780673\n",
      "Iter 800, loss 0.0006058764411136508\n",
      "Iter 900, loss 0.0006423317245207727\n",
      "Iter 1000, loss 0.0006848612101748586\n",
      "Iter 1100, loss 0.0007344171172007918\n",
      "Iter 1200, loss 0.0007944526732899249\n",
      "Iter 1300, loss 0.000869253883138299\n",
      "Iter 1400, loss 0.0009629856795072556\n",
      "Iter 1500, loss 0.0010893370490521193\n",
      "Iter 1600, loss 0.0012828224571421742\n",
      "Iter 1700, loss 0.0016861044568940997\n",
      "Iter 1800, loss 0.0028145008254796267\n",
      "Iter 0, loss 0.000399033073335886\n",
      "Iter 100, loss 0.0004120216181036085\n",
      "Iter 200, loss 0.0004260824352968484\n",
      "Iter 300, loss 0.0004418112221173942\n",
      "Iter 400, loss 0.0004592079494614154\n",
      "Iter 500, loss 0.00047839165199548006\n",
      "Iter 600, loss 0.0004993622424080968\n",
      "Iter 700, loss 0.0005227153305895627\n",
      "Iter 800, loss 0.0005489272880367935\n",
      "Iter 900, loss 0.0005782362422905862\n",
      "Iter 1000, loss 0.0006114759016782045\n",
      "Iter 1100, loss 0.0006498370785266161\n",
      "Iter 1200, loss 0.0006939148879610002\n",
      "Iter 1300, loss 0.0007454953738488257\n",
      "Iter 1400, loss 0.0008062449633143842\n",
      "Iter 1500, loss 0.0008799732895568013\n",
      "Iter 1600, loss 0.0009747759322635829\n",
      "Iter 1700, loss 0.0011026738211512566\n",
      "Iter 1800, loss 0.001292585046030581\n",
      "Iter 1900, loss 0.0016764646861702204\n",
      "Iter 2000, loss 0.0028089135885238647\n",
      "Iter 0, loss 0.00026556302327662706\n",
      "Iter 100, loss 0.0002694958820939064\n",
      "Iter 200, loss 0.00027366707217879593\n",
      "Iter 300, loss 0.0002779574424494058\n",
      "Iter 400, loss 0.0002824861148837954\n",
      "Iter 500, loss 0.00028713393840007484\n",
      "Iter 600, loss 0.00029202012228779495\n",
      "Iter 700, loss 0.0002971446083392948\n",
      "Iter 800, loss 0.0003023882454726845\n",
      "Iter 900, loss 0.00030787018476985395\n",
      "Iter 1000, loss 0.0003137096355203539\n",
      "Iter 1100, loss 0.0003197873884346336\n",
      "Iter 1200, loss 0.00032634177478030324\n",
      "Iter 1300, loss 0.0003331344632897526\n",
      "Iter 1400, loss 0.0003404037852305919\n",
      "Iter 1500, loss 0.00034814971149899065\n",
      "Iter 1600, loss 0.00035637227119877934\n",
      "Iter 1700, loss 0.00036507140612229705\n",
      "Iter 1800, loss 0.00037424711626954377\n",
      "Iter 1900, loss 0.0003840185818262398\n",
      "Iter 2000, loss 0.0003946240758523345\n",
      "Iter 2100, loss 0.0004060635983478278\n",
      "Iter 2200, loss 0.00041845624218694866\n",
      "Iter 2300, loss 0.0004319211875554174\n",
      "Iter 2400, loss 0.00044645831803791225\n",
      "Iter 2500, loss 0.0004621868138201535\n",
      "Iter 2600, loss 0.00047958316281437874\n",
      "Iter 2700, loss 0.0004987664869986475\n",
      "Iter 2800, loss 0.000519617460668087\n",
      "Iter 2900, loss 0.000542493537068367\n",
      "Iter 3000, loss 0.0005678709712810814\n",
      "Iter 3100, loss 0.0005963454605080187\n",
      "Iter 3200, loss 0.00062851223628968\n",
      "Iter 3300, loss 0.0006650857976637781\n",
      "Iter 3400, loss 0.0007066613179631531\n",
      "Iter 3500, loss 0.0007541911327280104\n",
      "Iter 3600, loss 0.000813034363090992\n",
      "Iter 3700, loss 0.0008859285153448582\n",
      "Iter 3800, loss 0.0009779914980754256\n",
      "Iter 3900, loss 0.0011035073548555374\n",
      "Iter 4000, loss 0.001290918211452663\n",
      "Iter 4100, loss 0.0016815820708870888\n",
      "Iter 4200, loss 0.002637005876749754\n",
      "Iter 0, loss 0.0006588910473510623\n",
      "Iter 100, loss 0.0007021345663815737\n",
      "Iter 200, loss 0.0007525234250351787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 300, loss 0.000812915270216763\n",
      "Iter 400, loss 0.0008859285153448582\n",
      "Iter 500, loss 0.0009773960337042809\n",
      "Iter 600, loss 0.0010987442219629884\n",
      "Iter 700, loss 0.0012679402716457844\n",
      "Iter 800, loss 0.001549235312268138\n",
      "Iter 900, loss 0.00225623045116663\n",
      "Iter 1000, loss 0.021913422271609306\n",
      "Iter 0, loss 0.00020215852418914437\n",
      "Iter 100, loss 0.00020418466010596603\n",
      "Iter 200, loss 0.00020632999076042324\n",
      "Iter 300, loss 0.00020859450160060078\n",
      "Iter 400, loss 0.0002108589978888631\n",
      "Iter 500, loss 0.0002131234941771254\n",
      "Iter 600, loss 0.00021562635083682835\n",
      "Iter 700, loss 0.00021801002731081098\n",
      "Iter 800, loss 0.00022063204960431904\n",
      "Iter 900, loss 0.0002232540718978271\n",
      "Iter 1000, loss 0.00022599527437705547\n",
      "Iter 1100, loss 0.00022885564249008894\n",
      "Iter 1200, loss 0.0002317160106031224\n",
      "Iter 1300, loss 0.0002348147245356813\n",
      "Iter 1400, loss 0.0002379134384682402\n",
      "Iter 1500, loss 0.00024125049822032452\n",
      "Iter 1600, loss 0.0002445875434204936\n",
      "Iter 1700, loss 0.00024816294899210334\n",
      "Iter 1800, loss 0.0002517383254598826\n",
      "Iter 1900, loss 0.00025555206229910254\n",
      "Iter 2000, loss 0.00025948495022021234\n",
      "Iter 2100, loss 0.0002636561985127628\n",
      "Iter 2200, loss 0.00026794656878337264\n",
      "Iter 2300, loss 0.0002723561483435333\n",
      "Iter 2400, loss 0.000277123210253194\n",
      "Iter 2500, loss 0.0002821285743266344\n",
      "Iter 2600, loss 0.00028725311858579516\n",
      "Iter 2700, loss 0.00029273517429828644\n",
      "Iter 2800, loss 0.0002983363519888371\n",
      "Iter 2900, loss 0.0003044141922146082\n",
      "Iter 3000, loss 0.0003106111544184387\n",
      "Iter 3100, loss 0.00031728477915748954\n",
      "Iter 3200, loss 0.00032443503732793033\n",
      "Iter 3300, loss 0.00033206192892976105\n",
      "Iter 3400, loss 0.0003401654539629817\n",
      "Iter 3500, loss 0.00034874555421993136\n",
      "Iter 3600, loss 0.0003579214389901608\n",
      "Iter 3700, loss 0.0003675738989841193\n",
      "Iter 3800, loss 0.0003780603874474764\n",
      "Iter 3900, loss 0.0003890234511345625\n",
      "Iter 4000, loss 0.00040070133400149643\n",
      "Iter 4100, loss 0.00041345154750160873\n",
      "Iter 4200, loss 0.000427274004323408\n",
      "Iter 4300, loss 0.000442407006630674\n",
      "Iter 4400, loss 0.00045908879837952554\n",
      "Iter 4500, loss 0.00047791501856409013\n",
      "Iter 4600, loss 0.0004984090337529778\n",
      "Iter 4700, loss 0.0005214046686887741\n",
      "Iter 4800, loss 0.0005476167425513268\n",
      "Iter 4900, loss 0.0005770448478870094\n",
      "Iter 5000, loss 0.0006107610533945262\n",
      "Iter 5100, loss 0.0006513857515528798\n",
      "Iter 5200, loss 0.0006996329175308347\n",
      "Iter 5300, loss 0.0007569308509118855\n",
      "Iter 5400, loss 0.0008266131044365466\n",
      "Iter 5500, loss 0.0009168949909508228\n",
      "Iter 5600, loss 0.0010387268848717213\n",
      "Iter 5700, loss 0.0012076949933543801\n",
      "Iter 5800, loss 0.0015010291244834661\n",
      "Iter 5900, loss 0.002279661362990737\n",
      "Iter 0, loss 0.0005911033367738128\n",
      "Iter 100, loss 0.0006282739923335612\n",
      "Iter 200, loss 0.0006713996990583837\n",
      "Iter 300, loss 0.0007217901293188334\n",
      "Iter 400, loss 0.0007825411157682538\n",
      "Iter 500, loss 0.0008574623498134315\n",
      "Iter 600, loss 0.0009540535393171012\n",
      "Iter 700, loss 0.0010838593589141965\n",
      "Iter 800, loss 0.0012740122620016336\n",
      "Iter 900, loss 0.0016349294455721974\n",
      "Iter 1000, loss 0.0026925283018499613\n",
      "Iter 0, loss 0.0003420721332076937\n",
      "Iter 100, loss 0.0003510097449179739\n",
      "Iter 200, loss 0.00036054308293387294\n",
      "Iter 300, loss 0.00037079135654494166\n",
      "Iter 400, loss 0.0003817544784396887\n",
      "Iter 500, loss 0.0003936707798857242\n",
      "Iter 600, loss 0.00040665941196493804\n",
      "Iter 700, loss 0.0004207202873658389\n",
      "Iter 800, loss 0.000436091679148376\n",
      "Iter 900, loss 0.0004528927383944392\n",
      "Iter 1000, loss 0.00047124247066676617\n",
      "Iter 1100, loss 0.0004914983292110264\n",
      "Iter 1200, loss 0.0005144941387698054\n",
      "Iter 1300, loss 0.0005409446312114596\n",
      "Iter 1400, loss 0.0005708495154976845\n",
      "Iter 1500, loss 0.0006050424999557436\n",
      "Iter 1600, loss 0.0006447143969126046\n",
      "Iter 1700, loss 0.0006908176001161337\n",
      "Iter 1800, loss 0.0007453762227669358\n",
      "Iter 1900, loss 0.0008111285860650241\n",
      "Iter 2000, loss 0.0008946230518631637\n",
      "Iter 2100, loss 0.0010008569806814194\n",
      "Iter 2200, loss 0.00114292127545923\n",
      "Iter 2300, loss 0.001378777320496738\n",
      "Iter 2400, loss 0.0019152885070070624\n",
      "Iter 2500, loss 0.003699365770444274\n",
      "Iter 0, loss 0.0003177614707965404\n",
      "Iter 100, loss 0.00032491172896698117\n",
      "Iter 200, loss 0.00033241944038309157\n",
      "Iter 300, loss 0.00034028460504487157\n",
      "Iter 400, loss 0.0003488647344056517\n",
      "Iter 500, loss 0.0003580405900720507\n",
      "Iter 600, loss 0.0003678122302517295\n",
      "Iter 700, loss 0.0003781795676331967\n",
      "Iter 800, loss 0.00038938093348406255\n",
      "Iter 900, loss 0.00040141629870049655\n",
      "Iter 1000, loss 0.0004142856632824987\n",
      "Iter 1100, loss 0.00042834642226807773\n",
      "Iter 1200, loss 0.0004435985756572336\n",
      "Iter 1300, loss 0.00046039948938414454\n",
      "Iter 1400, loss 0.0004786299541592598\n",
      "Iter 1500, loss 0.0004987664869986475\n",
      "Iter 1600, loss 0.0005214046686887741\n",
      "Iter 1700, loss 0.000546425289940089\n",
      "Iter 1800, loss 0.0005741854547522962\n",
      "Iter 1900, loss 0.0006055190460756421\n",
      "Iter 2000, loss 0.0006412595394067466\n",
      "Iter 2100, loss 0.0006825978052802384\n",
      "Iter 2200, loss 0.0007306052139028907\n",
      "Iter 2300, loss 0.0007871866691857576\n",
      "Iter 2400, loss 0.0008559139096178114\n",
      "Iter 2500, loss 0.0009427393670193851\n",
      "Iter 2600, loss 0.0010562323732301593\n",
      "Iter 2700, loss 0.0012151960982009768\n",
      "Iter 2800, loss 0.0014805557439103723\n",
      "Iter 2900, loss 0.0021431115455925465\n",
      "Iter 3000, loss 0.006718546152114868\n",
      "Iter 0, loss 0.00014840454969089478\n",
      "Iter 100, loss 0.00014911970356479287\n",
      "Iter 200, loss 0.0001497156627010554\n",
      "Iter 300, loss 0.00015043080202303827\n",
      "Iter 400, loss 0.00015114595589693636\n",
      "Iter 500, loss 0.00015186110977083445\n",
      "Iter 600, loss 0.00015245705435518175\n",
      "Iter 700, loss 0.00015317220822907984\n",
      "Iter 800, loss 0.0001538873475510627\n",
      "Iter 900, loss 0.00015472168161068112\n",
      "Iter 1000, loss 0.0001554368354845792\n",
      "Iter 1100, loss 0.00015615197480656207\n",
      "Iter 1200, loss 0.00015686711412854493\n",
      "Iter 1300, loss 0.00015758226800244302\n",
      "Iter 1400, loss 0.00015841660206206143\n",
      "Iter 1500, loss 0.0001591317413840443\n",
      "Iter 1600, loss 0.0001599660754436627\n",
      "Iter 1700, loss 0.00016068121476564556\n",
      "Iter 1800, loss 0.00016151554882526398\n",
      "Iter 1900, loss 0.00016234986833296716\n",
      "Iter 2000, loss 0.00016318420239258558\n",
      "Iter 2100, loss 0.000164018536452204\n",
      "Iter 2200, loss 0.00016485285595990717\n",
      "Iter 2300, loss 0.0001656871900195256\n",
      "Iter 2400, loss 0.00016664070426486433\n",
      "Iter 2500, loss 0.0001674750237725675\n",
      "Iter 2600, loss 0.00016842853801790625\n",
      "Iter 2700, loss 0.00016926287207752466\n",
      "Iter 2800, loss 0.00017021637177094817\n",
      "Iter 2900, loss 0.0001711698860162869\n",
      "Iter 3000, loss 0.00017212340026162565\n",
      "Iter 3100, loss 0.00017307691450696439\n",
      "Iter 3200, loss 0.00017414960893802345\n",
      "Iter 3300, loss 0.00017510310863144696\n",
      "Iter 3400, loss 0.00017617580306250602\n",
      "Iter 3500, loss 0.00017712931730784476\n",
      "Iter 3600, loss 0.00017820201173890382\n",
      "Iter 3700, loss 0.00017927470616996288\n",
      "Iter 3800, loss 0.00018046658078674227\n",
      "Iter 3900, loss 0.00018153927521780133\n",
      "Iter 4000, loss 0.00018273114983458072\n",
      "Iter 4100, loss 0.00018380382971372455\n",
      "Iter 4200, loss 0.00018499570433050394\n",
      "Iter 4300, loss 0.00018618757894728333\n",
      "Iter 4400, loss 0.00018737945356406271\n",
      "Iter 4500, loss 0.00018869050836656243\n",
      "Iter 4600, loss 0.00018988236843142658\n",
      "Iter 4700, loss 0.0001911934232339263\n",
      "Iter 4800, loss 0.000192504478036426\n",
      "Iter 4900, loss 0.0001938155182870105\n",
      "Iter 5000, loss 0.0001951265730895102\n",
      "Iter 5100, loss 0.000196556793525815\n",
      "Iter 5200, loss 0.00019798702851403505\n",
      "Iter 5300, loss 0.00019941726350225508\n",
      "Iter 5400, loss 0.0002008474839385599\n",
      "Iter 5500, loss 0.00020239688456058502\n",
      "Iter 5600, loss 0.00020382710499688983\n",
      "Iter 5700, loss 0.00020549570035655051\n",
      "Iter 5800, loss 0.00020704510097857565\n",
      "Iter 5900, loss 0.00020859450160060078\n",
      "Iter 6000, loss 0.00021026308240834624\n",
      "Iter 6100, loss 0.00021205084340181202\n",
      "Iter 6200, loss 0.00021371940965764225\n",
      "Iter 6300, loss 0.00021550717065110803\n",
      "Iter 6400, loss 0.0002172949316445738\n",
      "Iter 6500, loss 0.00021920185827184469\n",
      "Iter 6600, loss 0.00022110878489911556\n",
      "Iter 6700, loss 0.00022301571152638644\n",
      "Iter 6800, loss 0.00022504181833937764\n",
      "Iter 6900, loss 0.00022706791060045362\n",
      "Iter 7000, loss 0.00022909401741344482\n",
      "Iter 7100, loss 0.00023123928986024112\n",
      "Iter 7200, loss 0.00023338454775512218\n",
      "Iter 7300, loss 0.0002356490003876388\n",
      "Iter 7400, loss 0.0002379134384682402\n",
      "Iter 7500, loss 0.00024029705673456192\n",
      "Iter 7600, loss 0.0002426806604489684\n",
      "Iter 7700, loss 0.0002451834443490952\n",
      "Iter 7800, loss 0.00024768622824922204\n",
      "Iter 7900, loss 0.0002503081923350692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8000, loss 0.0002530493075028062\n",
      "Iter 8100, loss 0.0002557904226705432\n",
      "Iter 8200, loss 0.0002585315378382802\n",
      "Iter 8300, loss 0.0002613918040879071\n",
      "Iter 8400, loss 0.0002643712505232543\n",
      "Iter 8500, loss 0.0002674698771443218\n",
      "Iter 8600, loss 0.0002706876548472792\n",
      "Iter 8700, loss 0.0002739054325502366\n",
      "Iter 8800, loss 0.00027736154152080417\n",
      "Iter 8900, loss 0.00028081765049137175\n",
      "Iter 9000, loss 0.00028451209072954953\n",
      "Iter 9100, loss 0.0002882065309677273\n",
      "Iter 9200, loss 0.00029213930247351527\n",
      "Iter 9300, loss 0.000296310376143083\n",
      "Iter 9400, loss 0.00030048147891648114\n",
      "Iter 9500, loss 0.0003047717036679387\n",
      "Iter 9600, loss 0.0003093002596870065\n",
      "Iter 9700, loss 0.00031394799589179456\n",
      "Iter 9800, loss 0.00031883400515653193\n",
      "Iter 9900, loss 0.0003238391946069896\n",
      "Iter 0, loss 0.00042524831951595843\n",
      "Iter 100, loss 0.0004383556661196053\n",
      "Iter 200, loss 0.0004524161049630493\n",
      "Iter 300, loss 0.0004676678800024092\n",
      "Iter 400, loss 0.0004844683862756938\n",
      "Iter 500, loss 0.0005030558677390218\n",
      "Iter 600, loss 0.0005237876321189106\n",
      "Iter 700, loss 0.0005465444410219789\n",
      "Iter 800, loss 0.0005721600609831512\n",
      "Iter 900, loss 0.0006009918288327754\n",
      "Iter 1000, loss 0.000634111522231251\n",
      "Iter 1100, loss 0.00067223358200863\n",
      "Iter 1200, loss 0.000715833914000541\n",
      "Iter 1300, loss 0.0007665794692002237\n",
      "Iter 1400, loss 0.0008266131044365466\n",
      "Iter 1500, loss 0.0008993871742859483\n",
      "Iter 1600, loss 0.0009909725049510598\n",
      "Iter 1700, loss 0.0011087467428296804\n",
      "Iter 1800, loss 0.0012660353677347302\n",
      "Iter 1900, loss 0.0014975772937759757\n",
      "Iter 2000, loss 0.001904699020087719\n",
      "Iter 2100, loss 0.003295112634077668\n",
      "Iter 0, loss 0.001255677198059857\n",
      "Iter 100, loss 0.0015706595731899142\n",
      "Iter 200, loss 0.002469706116244197\n",
      "Iter 0, loss 0.0018710264703258872\n",
      "Iter 100, loss 0.0035359261091798544\n",
      "Iter 0, loss 0.0003665013937279582\n",
      "Iter 100, loss 0.0003768687602132559\n",
      "Iter 200, loss 0.00038795097498223186\n",
      "Iter 300, loss 0.0004001055203843862\n",
      "Iter 400, loss 0.00041333239641971886\n",
      "Iter 500, loss 0.00042775063775479794\n",
      "Iter 600, loss 0.00044347942457534373\n",
      "Iter 700, loss 0.00046075694262981415\n",
      "Iter 800, loss 0.0004801789182238281\n",
      "Iter 900, loss 0.000501983508002013\n",
      "Iter 1000, loss 0.0005261705373413861\n",
      "Iter 1100, loss 0.000553335587028414\n",
      "Iter 1200, loss 0.00058466981863603\n",
      "Iter 1300, loss 0.0006214833119884133\n",
      "Iter 1400, loss 0.0006641327636316419\n",
      "Iter 1500, loss 0.0007150000892579556\n",
      "Iter 1600, loss 0.00077622797107324\n",
      "Iter 1700, loss 0.000849601230584085\n",
      "Iter 1800, loss 0.0009433348895981908\n",
      "Iter 1900, loss 0.0010699268896132708\n",
      "Iter 2000, loss 0.0012510338565334678\n",
      "Iter 2100, loss 0.001553639187477529\n",
      "Iter 2200, loss 0.002411079127341509\n",
      "Iter 0, loss 0.0003163314249832183\n",
      "Iter 100, loss 0.00032264748006127775\n",
      "Iter 200, loss 0.0003293210465926677\n",
      "Iter 300, loss 0.00033623288618400693\n",
      "Iter 400, loss 0.00034350217902101576\n",
      "Iter 500, loss 0.00035124807618558407\n",
      "Iter 600, loss 0.0003594706067815423\n",
      "Iter 700, loss 0.00036816971260122955\n",
      "Iter 800, loss 0.0003773453936446458\n",
      "Iter 900, loss 0.00038723601028323174\n",
      "Iter 1000, loss 0.0003978414461016655\n",
      "Iter 1100, loss 0.00040951924165710807\n",
      "Iter 1200, loss 0.0004220310365781188\n",
      "Iter 1300, loss 0.00043561504571698606\n",
      "Iter 1400, loss 0.0004503904783632606\n",
      "Iter 1500, loss 0.0004664763400796801\n",
      "Iter 1600, loss 0.00048411093303002417\n",
      "Iter 1700, loss 0.0005038899253122509\n",
      "Iter 1800, loss 0.0005260513862594962\n",
      "Iter 1900, loss 0.0005504761938937008\n",
      "Iter 2000, loss 0.0005778788472525775\n",
      "Iter 2100, loss 0.0006087357178330421\n",
      "Iter 2200, loss 0.0006436422117985785\n",
      "Iter 2300, loss 0.0006836699321866035\n",
      "Iter 2400, loss 0.000730247818864882\n",
      "Iter 2500, loss 0.0007859955076128244\n",
      "Iter 2600, loss 0.0008521024719811976\n",
      "Iter 2700, loss 0.0009327351581305265\n",
      "Iter 2800, loss 0.001036107074469328\n",
      "Iter 2900, loss 0.001177094760350883\n",
      "Iter 3000, loss 0.0013866343069821596\n",
      "Iter 3100, loss 0.0017989181214943528\n",
      "Iter 3200, loss 0.003068265039473772\n",
      "Iter 0, loss 0.0014542490243911743\n",
      "Iter 100, loss 0.002026410773396492\n",
      "Iter 200, loss 0.004433208145201206\n",
      "Iter 0, loss 0.00031680811662226915\n",
      "Iter 100, loss 0.0003237200144212693\n",
      "Iter 200, loss 0.00033098942367359996\n",
      "Iter 300, loss 0.0003387354372534901\n",
      "Iter 400, loss 0.0003469580551609397\n",
      "Iter 500, loss 0.00035565727739594877\n",
      "Iter 600, loss 0.0003649522550404072\n",
      "Iter 700, loss 0.00037496211007237434\n",
      "Iter 800, loss 0.0003856868715956807\n",
      "Iter 900, loss 0.0003971264814026654\n",
      "Iter 1000, loss 0.00040928093949332833\n",
      "Iter 1100, loss 0.0004226268210913986\n",
      "Iter 1200, loss 0.00043704494601115584\n",
      "Iter 1300, loss 0.0004530118894763291\n",
      "Iter 1400, loss 0.00047064671525731683\n",
      "Iter 1500, loss 0.000490068516228348\n",
      "Iter 1600, loss 0.0005113962688483298\n",
      "Iter 1700, loss 0.0005360596696846187\n",
      "Iter 1800, loss 0.0005639393348246813\n",
      "Iter 1900, loss 0.0005959880072623491\n",
      "Iter 2000, loss 0.0006323245470412076\n",
      "Iter 2100, loss 0.0006742588011547923\n",
      "Iter 2200, loss 0.0007236960809677839\n",
      "Iter 2300, loss 0.0007828985108062625\n",
      "Iter 2400, loss 0.000856628583278507\n",
      "Iter 2500, loss 0.0009483369067311287\n",
      "Iter 2600, loss 0.0010671879863366485\n",
      "Iter 2700, loss 0.0012338890228420496\n",
      "Iter 2800, loss 0.0014896021457388997\n",
      "Iter 2900, loss 0.002040686784312129\n",
      "Iter 3000, loss 0.004480798728764057\n",
      "Iter 0, loss 0.00022957073815632612\n",
      "Iter 100, loss 0.0002325502864550799\n",
      "Iter 200, loss 0.0002356490003876388\n",
      "Iter 300, loss 0.0002388668799540028\n",
      "Iter 400, loss 0.00024232311989180744\n",
      "Iter 500, loss 0.00024577934527769685\n",
      "Iter 600, loss 0.0002493547508493066\n",
      "Iter 700, loss 0.0002531684876885265\n",
      "Iter 800, loss 0.000256982195423916\n",
      "Iter 900, loss 0.0002610342635307461\n",
      "Iter 1000, loss 0.00026520551182329655\n",
      "Iter 1100, loss 0.0002696150622796267\n",
      "Iter 1200, loss 0.00027414379292167723\n",
      "Iter 1300, loss 0.00027891082572750747\n",
      "Iter 1400, loss 0.00028379703871905804\n",
      "Iter 1500, loss 0.0002889215829782188\n",
      "Iter 1600, loss 0.0002942844294011593\n",
      "Iter 1700, loss 0.0003000047872774303\n",
      "Iter 1800, loss 0.00030596344731748104\n",
      "Iter 1900, loss 0.0003123987407889217\n",
      "Iter 2000, loss 0.0003194298769813031\n",
      "Iter 2100, loss 0.00032693761750124395\n",
      "Iter 2200, loss 0.00033480284037068486\n",
      "Iter 2300, loss 0.00034314466756768525\n",
      "Iter 2400, loss 0.0003519630990922451\n",
      "Iter 2500, loss 0.00036125810584053397\n",
      "Iter 2600, loss 0.0003711488388944417\n",
      "Iter 2700, loss 0.0003816353273577988\n",
      "Iter 2800, loss 0.00039295581518672407\n",
      "Iter 2900, loss 0.0004051103023812175\n",
      "Iter 3000, loss 0.00041845624218694866\n",
      "Iter 3100, loss 0.00043299360550008714\n",
      "Iter 3200, loss 0.0004487222759053111\n",
      "Iter 3300, loss 0.00046588058467023075\n",
      "Iter 3400, loss 0.0004848258395213634\n",
      "Iter 3500, loss 0.000505677133332938\n",
      "Iter 3600, loss 0.0005286726518534124\n",
      "Iter 3700, loss 0.0005544078885577619\n",
      "Iter 3800, loss 0.0005833592731505632\n",
      "Iter 3900, loss 0.0006163604557514191\n",
      "Iter 4000, loss 0.000654602306894958\n",
      "Iter 4100, loss 0.0006987990345805883\n",
      "Iter 4200, loss 0.0007521660882048309\n",
      "Iter 4300, loss 0.0008164886385202408\n",
      "Iter 4400, loss 0.0008978387922979891\n",
      "Iter 4500, loss 0.0010032388381659985\n",
      "Iter 4600, loss 0.0011491130571812391\n",
      "Iter 4700, loss 0.0013859200989827514\n",
      "Iter 4800, loss 0.0019425348145887256\n",
      "Iter 4900, loss 0.0037802441511303186\n",
      "Iter 0, loss 0.00012087091454304755\n",
      "Iter 100, loss 0.00012134769349358976\n",
      "Iter 200, loss 0.00012182447244413197\n",
      "Iter 300, loss 0.00012230125139467418\n",
      "Iter 400, loss 0.0001227780303452164\n",
      "Iter 500, loss 0.00012337400403339416\n",
      "Iter 600, loss 0.00012385078298393637\n",
      "Iter 700, loss 0.00012432756193447858\n",
      "Iter 800, loss 0.0001248043408850208\n",
      "Iter 900, loss 0.00012540031457319856\n",
      "Iter 1000, loss 0.00012587709352374077\n",
      "Iter 1100, loss 0.0001264730526600033\n",
      "Iter 1200, loss 0.00012694983161054552\n",
      "Iter 1300, loss 0.00012754580529872328\n",
      "Iter 1400, loss 0.0001280225842492655\n",
      "Iter 1500, loss 0.00012861855793744326\n",
      "Iter 1600, loss 0.0001292145170737058\n",
      "Iter 1700, loss 0.00012981049076188356\n",
      "Iter 1800, loss 0.00013040646445006132\n",
      "Iter 1900, loss 0.0001308832288486883\n",
      "Iter 2000, loss 0.00013147920253686607\n",
      "Iter 2100, loss 0.00013207517622504383\n",
      "Iter 2200, loss 0.00013279033009894192\n",
      "Iter 2300, loss 0.0001333863037871197\n",
      "Iter 2400, loss 0.00013398226292338222\n",
      "Iter 2500, loss 0.00013457823661156\n",
      "Iter 2600, loss 0.00013517419574782252\n",
      "Iter 2700, loss 0.0001358893496217206\n",
      "Iter 2800, loss 0.00013648532330989838\n",
      "Iter 2900, loss 0.00013720047718379647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3000, loss 0.00013779645087197423\n",
      "Iter 3100, loss 0.00013851160474587232\n",
      "Iter 3200, loss 0.0001392267586197704\n",
      "Iter 3300, loss 0.0001399419124936685\n",
      "Iter 3400, loss 0.00014053787162993103\n",
      "Iter 3500, loss 0.00014125302550382912\n",
      "Iter 3600, loss 0.0001419681793777272\n",
      "Iter 3700, loss 0.0001426833332516253\n",
      "Iter 3800, loss 0.00014351768186315894\n",
      "Iter 3900, loss 0.00014423283573705703\n",
      "Iter 4000, loss 0.00014494798961095512\n",
      "Iter 4100, loss 0.0001456631434848532\n",
      "Iter 4200, loss 0.00014649749209638685\n",
      "Iter 4300, loss 0.0001472126314183697\n",
      "Iter 4400, loss 0.00014804698002990335\n",
      "Iter 4500, loss 0.00014888131408952177\n",
      "Iter 4600, loss 0.00014959646796341985\n",
      "Iter 4700, loss 0.00015043080202303827\n",
      "Iter 4800, loss 0.0001512651506345719\n",
      "Iter 4900, loss 0.00015209948469419032\n",
      "Iter 5000, loss 0.00015293381875380874\n",
      "Iter 5100, loss 0.00015376816736534238\n",
      "Iter 5200, loss 0.00015472168161068112\n",
      "Iter 5300, loss 0.00015555603022221476\n",
      "Iter 5400, loss 0.0001565095444675535\n",
      "Iter 5500, loss 0.0001573438785271719\n",
      "Iter 5600, loss 0.00015829740732442588\n",
      "Iter 5700, loss 0.00015925093612167984\n",
      "Iter 5800, loss 0.00016020445036701858\n",
      "Iter 5900, loss 0.00016115797916427255\n",
      "Iter 6000, loss 0.00016211149340961128\n",
      "Iter 6100, loss 0.00016306500765495002\n",
      "Iter 6200, loss 0.0001641377166379243\n",
      "Iter 6300, loss 0.0001652104256208986\n",
      "Iter 6400, loss 0.0001662831346038729\n",
      "Iter 6500, loss 0.00016735584358684719\n",
      "Iter 6600, loss 0.00016842853801790625\n",
      "Iter 6700, loss 0.00016950124700088054\n",
      "Iter 6800, loss 0.00017069313616957515\n",
      "Iter 6900, loss 0.00017188502533826977\n",
      "Iter 7000, loss 0.00017295771976932883\n",
      "Iter 7100, loss 0.00017426878912374377\n",
      "Iter 7200, loss 0.0001754606782924384\n",
      "Iter 7300, loss 0.000176652567461133\n",
      "Iter 7400, loss 0.00017796363681554794\n",
      "Iter 7500, loss 0.00017927470616996288\n",
      "Iter 7600, loss 0.0001805857609724626\n",
      "Iter 7700, loss 0.00018189683032687753\n",
      "Iter 7800, loss 0.0001833270798670128\n",
      "Iter 7900, loss 0.00018475732940714806\n",
      "Iter 8000, loss 0.00018618757894728333\n",
      "Iter 8100, loss 0.00018761781393550336\n",
      "Iter 8200, loss 0.00018916724366135895\n",
      "Iter 8300, loss 0.00019071667338721454\n",
      "Iter 8400, loss 0.00019226610311307013\n",
      "Iter 8500, loss 0.0001938155182870105\n",
      "Iter 8600, loss 0.0001954841281985864\n",
      "Iter 8700, loss 0.00019727191829588264\n",
      "Iter 8800, loss 0.00019894051365554333\n",
      "Iter 8900, loss 0.00020072828920092434\n",
      "Iter 9000, loss 0.0002026352594839409\n",
      "Iter 9100, loss 0.0002044230350293219\n",
      "Iter 9200, loss 0.00020632999076042324\n",
      "Iter 9300, loss 0.00020823694649152458\n",
      "Iter 9400, loss 0.00021026308240834624\n",
      "Iter 9500, loss 0.00021228920377325267\n",
      "Iter 9600, loss 0.00021443451987579465\n",
      "Iter 9700, loss 0.0002165798214264214\n",
      "Iter 9800, loss 0.00021884430316276848\n",
      "Iter 9900, loss 0.00022110878489911556\n",
      "Iter 0, loss 0.0004385939973872155\n",
      "Iter 100, loss 0.00045503751607611775\n",
      "Iter 200, loss 0.00047338721924461424\n",
      "Iter 300, loss 0.0004936429904773831\n",
      "Iter 400, loss 0.0005162813467904925\n",
      "Iter 500, loss 0.000542493537068367\n",
      "Iter 600, loss 0.0005735897575505078\n",
      "Iter 700, loss 0.0006094505661167204\n",
      "Iter 800, loss 0.0006518622976727784\n",
      "Iter 900, loss 0.0007020154735073447\n",
      "Iter 1000, loss 0.0007624103454872966\n",
      "Iter 1100, loss 0.0008379285573028028\n",
      "Iter 1200, loss 0.0009355935617350042\n",
      "Iter 1300, loss 0.0010662352433428168\n",
      "Iter 1400, loss 0.0012554391287267208\n",
      "Iter 1500, loss 0.0016180293168872595\n",
      "Iter 1600, loss 0.002754348563030362\n",
      "Iter 0, loss 0.0002522150462027639\n",
      "Iter 100, loss 0.00025614796322770417\n",
      "Iter 200, loss 0.0002603192115202546\n",
      "Iter 300, loss 0.0002646096108946949\n",
      "Iter 400, loss 0.00026913834153674543\n",
      "Iter 500, loss 0.0002739054325502366\n",
      "Iter 600, loss 0.0002787916746456176\n",
      "Iter 700, loss 0.0002840353990904987\n",
      "Iter 800, loss 0.00028939827461726964\n",
      "Iter 900, loss 0.00029523781267926097\n",
      "Iter 1000, loss 0.0003013156820088625\n",
      "Iter 1100, loss 0.00030787018476985395\n",
      "Iter 1200, loss 0.0003147821989841759\n",
      "Iter 1300, loss 0.00032217081752605736\n",
      "Iter 1400, loss 0.0003300360403954983\n",
      "Iter 1500, loss 0.0003383779258001596\n",
      "Iter 1600, loss 0.0003474347176961601\n",
      "Iter 1700, loss 0.0003573255962692201\n",
      "Iter 1800, loss 0.00036816971260122955\n",
      "Iter 1900, loss 0.0003798478574026376\n",
      "Iter 2000, loss 0.0003924791526515037\n",
      "Iter 2100, loss 0.000406301929615438\n",
      "Iter 2200, loss 0.00042143522296100855\n",
      "Iter 2300, loss 0.0004377598816063255\n",
      "Iter 2400, loss 0.0004555141495075077\n",
      "Iter 2500, loss 0.00047505536349490285\n",
      "Iter 2600, loss 0.00049650261644274\n",
      "Iter 2700, loss 0.0005204515182413161\n",
      "Iter 2800, loss 0.0005472592893056571\n",
      "Iter 2900, loss 0.000577402301132679\n",
      "Iter 3000, loss 0.0006118332967162132\n",
      "Iter 3100, loss 0.0006532918778248131\n",
      "Iter 3200, loss 0.0007034449372440577\n",
      "Iter 3300, loss 0.0007641970878466964\n",
      "Iter 3400, loss 0.0008403107640333474\n",
      "Iter 3500, loss 0.0009390473715029657\n",
      "Iter 3600, loss 0.0010756427654996514\n",
      "Iter 3700, loss 0.00129413278773427\n",
      "Iter 3800, loss 0.0018009409541264176\n",
      "Iter 3900, loss 0.003380419919267297\n",
      "Iter 0, loss 0.0006048041977919638\n",
      "Iter 100, loss 0.0006345880683511496\n",
      "Iter 200, loss 0.0006683023530058563\n",
      "Iter 300, loss 0.0007064230740070343\n",
      "Iter 400, loss 0.0007496645557694137\n",
      "Iter 500, loss 0.0008002892718650401\n",
      "Iter 600, loss 0.0008591298246756196\n",
      "Iter 700, loss 0.0009296386269852519\n",
      "Iter 800, loss 0.001016934053041041\n",
      "Iter 900, loss 0.001128513365983963\n",
      "Iter 1000, loss 0.0012767505832016468\n",
      "Iter 1100, loss 0.0015047191409394145\n",
      "Iter 1200, loss 0.0019620470702648163\n",
      "Iter 1300, loss 0.0032530506141483784\n",
      "Iter 0, loss 0.0002300474588992074\n",
      "Iter 100, loss 0.00023266946664080024\n",
      "Iter 200, loss 0.00023529145983047783\n",
      "Iter 300, loss 0.00023803261865396053\n",
      "Iter 400, loss 0.00024089295766316354\n",
      "Iter 500, loss 0.00024375328212045133\n",
      "Iter 600, loss 0.00024673278676345944\n",
      "Iter 700, loss 0.0002498314715921879\n",
      "Iter 800, loss 0.0002530493075028062\n",
      "Iter 900, loss 0.0002562671434134245\n",
      "Iter 1000, loss 0.00025960413040593266\n",
      "Iter 1100, loss 0.0002631794777698815\n",
      "Iter 1200, loss 0.0002668739762157202\n",
      "Iter 1300, loss 0.0002706876548472792\n",
      "Iter 1400, loss 0.0002746204845607281\n",
      "Iter 1500, loss 0.0002787916746456176\n",
      "Iter 1600, loss 0.00028308198670856655\n",
      "Iter 1700, loss 0.00028772983932867646\n",
      "Iter 1800, loss 0.0002926159941125661\n",
      "Iter 1900, loss 0.00029774048016406596\n",
      "Iter 2000, loss 0.00030310326837934554\n",
      "Iter 2100, loss 0.00030882356804795563\n",
      "Iter 2200, loss 0.0003147821989841759\n",
      "Iter 2300, loss 0.00032109825406223536\n",
      "Iter 2400, loss 0.000327652640407905\n",
      "Iter 2500, loss 0.00033444532891735435\n",
      "Iter 2600, loss 0.00034171465085819364\n",
      "Iter 2700, loss 0.0003493413969408721\n",
      "Iter 2800, loss 0.0003575639275368303\n",
      "Iter 2900, loss 0.000366263062460348\n",
      "Iter 3000, loss 0.00037555795279331505\n",
      "Iter 3100, loss 0.00038532938924618065\n",
      "Iter 3200, loss 0.00039569655200466514\n",
      "Iter 3300, loss 0.0004067785630468279\n",
      "Iter 3400, loss 0.00041845624218694866\n",
      "Iter 3500, loss 0.00043108707177452743\n",
      "Iter 3600, loss 0.000444551813416183\n",
      "Iter 3700, loss 0.00045908879837952554\n",
      "Iter 3800, loss 0.0004748170613311231\n",
      "Iter 3900, loss 0.0004919749335385859\n",
      "Iter 4000, loss 0.0005110388156026602\n",
      "Iter 4100, loss 0.0005324853118509054\n",
      "Iter 4200, loss 0.0005566716426983476\n",
      "Iter 4300, loss 0.0005837167263962328\n",
      "Iter 4400, loss 0.0006146925734356046\n",
      "Iter 4500, loss 0.0006504327175207436\n",
      "Iter 4600, loss 0.000691770575940609\n",
      "Iter 4700, loss 0.0007397775771096349\n",
      "Iter 4800, loss 0.0007968349382281303\n",
      "Iter 4900, loss 0.0008648469229228795\n",
      "Iter 5000, loss 0.0009484559996053576\n",
      "Iter 5100, loss 0.0010590903693810105\n",
      "Iter 5200, loss 0.001210433547385037\n",
      "Iter 5300, loss 0.0014381790533661842\n",
      "Iter 5400, loss 0.0019319456769153476\n",
      "Iter 5500, loss 0.0035536254290491343\n",
      "Iter 0, loss 0.00016544880054425448\n",
      "Iter 100, loss 0.00016675988445058465\n",
      "Iter 200, loss 0.0001679517881711945\n",
      "Iter 300, loss 0.00016926287207752466\n",
      "Iter 400, loss 0.00017045476124621928\n",
      "Iter 500, loss 0.00017176583060063422\n",
      "Iter 600, loss 0.0001731960946926847\n",
      "Iter 700, loss 0.00017450717859901488\n",
      "Iter 800, loss 0.00017593742813915014\n",
      "Iter 900, loss 0.00017736769223120064\n",
      "Iter 1000, loss 0.00017891713650897145\n",
      "Iter 1100, loss 0.00018034738604910672\n",
      "Iter 1200, loss 0.00018189683032687753\n",
      "Iter 1300, loss 0.00018356545479036868\n",
      "Iter 1400, loss 0.0001851148990681395\n",
      "Iter 1500, loss 0.0001867835089797154\n",
      "Iter 1600, loss 0.00018845213344320655\n",
      "Iter 1700, loss 0.0001902399235405028\n",
      "Iter 1800, loss 0.0001919085334520787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1900, loss 0.0001938155182870105\n",
      "Iter 2000, loss 0.00019560330838430673\n",
      "Iter 2100, loss 0.00019751029321923852\n",
      "Iter 2200, loss 0.00019941726350225508\n",
      "Iter 2300, loss 0.00020144341397099197\n",
      "Iter 2400, loss 0.00020358874462544918\n",
      "Iter 2500, loss 0.0002057340752799064\n",
      "Iter 2600, loss 0.00020787939138244838\n",
      "Iter 2700, loss 0.0002101439022226259\n",
      "Iter 2800, loss 0.00021252757869660854\n",
      "Iter 2900, loss 0.0002150304353563115\n",
      "Iter 3000, loss 0.00021753329201601446\n",
      "Iter 3100, loss 0.00022003613412380219\n",
      "Iter 3200, loss 0.0002227773511549458\n",
      "Iter 3300, loss 0.00022551853908225894\n",
      "Iter 3400, loss 0.00022837892174720764\n",
      "Iter 3500, loss 0.00023135847004596144\n",
      "Iter 3600, loss 0.0002343380037928\n",
      "Iter 3700, loss 0.0002374367177253589\n",
      "Iter 3800, loss 0.0002406545972917229\n",
      "Iter 3900, loss 0.00024399164249189198\n",
      "Iter 4000, loss 0.0002474478678777814\n",
      "Iter 4100, loss 0.00025102324434556067\n",
      "Iter 4200, loss 0.0002547178009990603\n",
      "Iter 4300, loss 0.0002586507180240005\n",
      "Iter 4400, loss 0.0002627027570270002\n",
      "Iter 4500, loss 0.0002668739762157202\n",
      "Iter 4600, loss 0.0002712835557758808\n",
      "Iter 4700, loss 0.0002759314374998212\n",
      "Iter 4800, loss 0.00028081765049137175\n",
      "Iter 4900, loss 0.0002858230145648122\n",
      "Iter 5000, loss 0.00029118589009158313\n",
      "Iter 5100, loss 0.0002967870968859643\n",
      "Iter 5200, loss 0.000302745756926015\n",
      "Iter 5300, loss 0.0003090619284193963\n",
      "Iter 5400, loss 0.00031573555315844715\n",
      "Iter 5500, loss 0.00032300499151460826\n",
      "Iter 5600, loss 0.0003307510633021593\n",
      "Iter 5700, loss 0.00033909291960299015\n",
      "Iter 5800, loss 0.00034791138023138046\n",
      "Iter 5900, loss 0.0003573255962692201\n",
      "Iter 6000, loss 0.0003673355677165091\n",
      "Iter 6100, loss 0.0003779412363655865\n",
      "Iter 6200, loss 0.00038938093348406255\n",
      "Iter 6300, loss 0.00040165462996810675\n",
      "Iter 6400, loss 0.00041500062798149884\n",
      "Iter 6500, loss 0.0004294188693165779\n",
      "Iter 6600, loss 0.00044526674901135266\n",
      "Iter 6700, loss 0.0004627825692296028\n",
      "Iter 6800, loss 0.0004820853646378964\n",
      "Iter 6900, loss 0.0005035324720665812\n",
      "Iter 7000, loss 0.0005277194431982934\n",
      "Iter 7100, loss 0.0005547653418034315\n",
      "Iter 7200, loss 0.0005858612130396068\n",
      "Iter 7300, loss 0.0006213641609065235\n",
      "Iter 7400, loss 0.0006624649395234883\n",
      "Iter 7500, loss 0.0007111880695447326\n",
      "Iter 7600, loss 0.0007696765242144465\n",
      "Iter 7700, loss 0.0008415018673986197\n",
      "Iter 7800, loss 0.0009336879593320191\n",
      "Iter 7900, loss 0.0010564705589786172\n",
      "Iter 8000, loss 0.0012288884026929736\n",
      "Iter 8100, loss 0.0015417367685586214\n",
      "Iter 8200, loss 0.0023981165140867233\n",
      "Iter 0, loss 9.476689592702314e-05\n",
      "Iter 100, loss 9.500529267825186e-05\n",
      "Iter 200, loss 9.512448741588742e-05\n",
      "Iter 300, loss 9.536288416711614e-05\n",
      "Iter 400, loss 9.560128091834486e-05\n",
      "Iter 500, loss 9.583967766957358e-05\n",
      "Iter 600, loss 9.60780744208023e-05\n",
      "Iter 700, loss 9.619726915843785e-05\n",
      "Iter 800, loss 9.643566590966657e-05\n",
      "Iter 900, loss 9.667406266089529e-05\n",
      "Iter 1000, loss 9.691245941212401e-05\n",
      "Iter 1100, loss 9.715084888739511e-05\n",
      "Iter 1200, loss 9.738924563862383e-05\n",
      "Iter 1300, loss 9.762764238985255e-05\n",
      "Iter 1400, loss 9.77468371274881e-05\n",
      "Iter 1500, loss 9.798523387871683e-05\n",
      "Iter 1600, loss 9.822363062994555e-05\n",
      "Iter 1700, loss 9.846202738117427e-05\n",
      "Iter 1800, loss 9.870042413240299e-05\n",
      "Iter 1900, loss 9.893881360767409e-05\n",
      "Iter 2000, loss 9.917721035890281e-05\n",
      "Iter 2100, loss 9.941560711013153e-05\n",
      "Iter 2200, loss 9.965400386136025e-05\n",
      "Iter 2300, loss 9.989239333663136e-05\n",
      "Iter 2400, loss 0.00010013079008786008\n",
      "Iter 2500, loss 0.0001003691868390888\n",
      "Iter 2600, loss 0.0001006075763143599\n",
      "Iter 2700, loss 0.00010084597306558862\n",
      "Iter 2800, loss 0.00010108436981681734\n",
      "Iter 2900, loss 0.00010132275929208845\n",
      "Iter 3000, loss 0.00010156115604331717\n",
      "Iter 3100, loss 0.00010179955279454589\n",
      "Iter 3200, loss 0.000102037942269817\n",
      "Iter 3300, loss 0.00010227633902104571\n",
      "Iter 3400, loss 0.00010251473577227443\n",
      "Iter 3500, loss 0.00010275312524754554\n",
      "Iter 3600, loss 0.00010299152199877426\n",
      "Iter 3700, loss 0.00010322991875000298\n",
      "Iter 3800, loss 0.00010358751023886725\n",
      "Iter 3900, loss 0.00010382589971413836\n",
      "Iter 4000, loss 0.00010406429646536708\n",
      "Iter 4100, loss 0.00010430268594063818\n",
      "Iter 4200, loss 0.0001045410826918669\n",
      "Iter 4300, loss 0.00010477947944309562\n",
      "Iter 4400, loss 0.00010513706365600228\n",
      "Iter 4500, loss 0.000105375460407231\n",
      "Iter 4600, loss 0.00010561384988250211\n",
      "Iter 4700, loss 0.00010585224663373083\n",
      "Iter 4800, loss 0.00010609064338495955\n",
      "Iter 4900, loss 0.00010644822759786621\n",
      "Iter 5000, loss 0.00010668662434909493\n",
      "Iter 5100, loss 0.0001070442158379592\n",
      "Iter 5200, loss 0.0001072826053132303\n",
      "Iter 5300, loss 0.00010752100206445903\n",
      "Iter 5400, loss 0.00010787858627736568\n",
      "Iter 5500, loss 0.0001081169830285944\n",
      "Iter 5600, loss 0.00010847456724150106\n",
      "Iter 5700, loss 0.00010871296399272978\n",
      "Iter 5800, loss 0.00010907054820563644\n",
      "Iter 5900, loss 0.00010930894495686516\n",
      "Iter 6000, loss 0.00010966652916977182\n",
      "Iter 6100, loss 0.00010990492592100054\n",
      "Iter 6200, loss 0.0001102625101339072\n",
      "Iter 6300, loss 0.00011062010162277147\n",
      "Iter 6400, loss 0.00011085849109804258\n",
      "Iter 6500, loss 0.00011121608258690685\n",
      "Iter 6600, loss 0.00011145447206217796\n",
      "Iter 6700, loss 0.00011181206355104223\n",
      "Iter 6800, loss 0.00011216964776394889\n",
      "Iter 6900, loss 0.00011240804451517761\n",
      "Iter 7000, loss 0.00011276562872808427\n",
      "Iter 7100, loss 0.00011312322021694854\n",
      "Iter 7200, loss 0.00011336160969221964\n",
      "Iter 7300, loss 0.0001137191939051263\n",
      "Iter 7400, loss 0.00011407678539399058\n",
      "Iter 7500, loss 0.00011443436960689723\n",
      "Iter 7600, loss 0.0001147919538198039\n",
      "Iter 7700, loss 0.00011503035057103261\n",
      "Iter 7800, loss 0.00011538793478393927\n",
      "Iter 7900, loss 0.00011574551899684593\n",
      "Iter 8000, loss 0.0001161031104857102\n",
      "Iter 8100, loss 0.00011646069469861686\n",
      "Iter 8200, loss 0.00011681827891152352\n",
      "Iter 8300, loss 0.00011717586312443018\n",
      "Iter 8400, loss 0.00011753345461329445\n",
      "Iter 8500, loss 0.00011789103882620111\n",
      "Iter 8600, loss 0.00011824862303910777\n",
      "Iter 8700, loss 0.00011860620725201443\n",
      "Iter 8800, loss 0.00011896379146492109\n",
      "Iter 8900, loss 0.00011932138295378536\n",
      "Iter 9000, loss 0.00011967896716669202\n",
      "Iter 9100, loss 0.00012003655137959868\n",
      "Iter 9200, loss 0.00012039413559250534\n",
      "Iter 9300, loss 0.000120751719805412\n",
      "Iter 9400, loss 0.00012110930401831865\n",
      "Iter 9500, loss 0.00012146688823122531\n",
      "Iter 9600, loss 0.00012182447244413197\n",
      "Iter 9700, loss 0.00012230125139467418\n",
      "Iter 9800, loss 0.00012265883560758084\n",
      "Iter 9900, loss 0.0001230164198204875\n",
      "Iter 0, loss 0.000835903687402606\n",
      "Iter 100, loss 0.000920825288631022\n",
      "Iter 200, loss 0.0010334871476516128\n",
      "Iter 300, loss 0.001189596951007843\n",
      "Iter 400, loss 0.0014229421503841877\n",
      "Iter 500, loss 0.0018932766979560256\n",
      "Iter 600, loss 0.0039527867920696735\n",
      "Iter 0, loss 0.008065156638622284\n",
      "Iter 0, loss 0.0009483369067311287\n",
      "Iter 100, loss 0.0010721894213929772\n",
      "Iter 200, loss 0.0012456761905923486\n",
      "Iter 300, loss 0.0015345951542258263\n",
      "Iter 400, loss 0.002297026105225086\n",
      "Iter 500, loss 0.5801042318344116\n",
      "Iter 0, loss 0.00014757021563127637\n",
      "Iter 100, loss 0.00014828535495325923\n",
      "Iter 200, loss 0.00014911970356479287\n",
      "Iter 300, loss 0.00014995403762441128\n",
      "Iter 400, loss 0.00015066919149830937\n",
      "Iter 500, loss 0.0001515035255579278\n",
      "Iter 600, loss 0.00015233787416946143\n",
      "Iter 700, loss 0.00015317220822907984\n",
      "Iter 800, loss 0.00015400654228869826\n",
      "Iter 900, loss 0.00015484087634831667\n",
      "Iter 1000, loss 0.00015567521040793508\n",
      "Iter 1100, loss 0.00015662873920518905\n",
      "Iter 1200, loss 0.00015746307326480746\n",
      "Iter 1300, loss 0.00015841660206206143\n",
      "Iter 1400, loss 0.00015925093612167984\n",
      "Iter 1500, loss 0.00016020445036701858\n",
      "Iter 1600, loss 0.00016115797916427255\n",
      "Iter 1700, loss 0.00016211149340961128\n",
      "Iter 1800, loss 0.00016306500765495002\n",
      "Iter 1900, loss 0.000164018536452204\n",
      "Iter 2000, loss 0.00016509123088326305\n",
      "Iter 2100, loss 0.00016604475968051702\n",
      "Iter 2200, loss 0.00016711745411157608\n",
      "Iter 2300, loss 0.00016819016309455037\n",
      "Iter 2400, loss 0.0001691436773398891\n",
      "Iter 2500, loss 0.00017021637177094817\n",
      "Iter 2600, loss 0.00017128908075392246\n",
      "Iter 2700, loss 0.00017248096992261708\n",
      "Iter 2800, loss 0.00017355366435367614\n",
      "Iter 2900, loss 0.0001746263587847352\n",
      "Iter 3000, loss 0.00017581824795342982\n",
      "Iter 3100, loss 0.0001770101225702092\n",
      "Iter 3200, loss 0.00017808281700126827\n",
      "Iter 3300, loss 0.00017927470616996288\n",
      "Iter 3400, loss 0.0001805857609724626\n",
      "Iter 3500, loss 0.0001817776501411572\n",
      "Iter 3600, loss 0.00018308870494365692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3700, loss 0.00018439977429807186\n",
      "Iter 3800, loss 0.00018571082910057157\n",
      "Iter 3900, loss 0.00018702188390307128\n",
      "Iter 4000, loss 0.000188332938705571\n",
      "Iter 4100, loss 0.00018988236843142658\n",
      "Iter 4200, loss 0.00019131260341964662\n",
      "Iter 4300, loss 0.00019274283840786666\n",
      "Iter 4400, loss 0.00019429226813372225\n",
      "Iter 4500, loss 0.0001958416833076626\n",
      "Iter 4600, loss 0.00019751029321923852\n",
      "Iter 4700, loss 0.00019905969384126365\n",
      "Iter 4800, loss 0.00020072828920092434\n",
      "Iter 4900, loss 0.00020251607929822057\n",
      "Iter 5000, loss 0.00020418466010596603\n",
      "Iter 5100, loss 0.00020597243565134704\n",
      "Iter 5200, loss 0.00020776021119672805\n",
      "Iter 5300, loss 0.00020954797219019383\n",
      "Iter 5400, loss 0.00021145492792129517\n",
      "Iter 5500, loss 0.00021336186910048127\n",
      "Iter 5600, loss 0.0002153879904653877\n",
      "Iter 5700, loss 0.00021741411183029413\n",
      "Iter 5800, loss 0.0002195594133809209\n",
      "Iter 5900, loss 0.0002217047003796324\n",
      "Iter 6000, loss 0.00022384998737834394\n",
      "Iter 6100, loss 0.00022623363474849612\n",
      "Iter 6200, loss 0.00022849810193292797\n",
      "Iter 6300, loss 0.0002308817347511649\n",
      "Iter 6400, loss 0.00023338454775512218\n",
      "Iter 6500, loss 0.00023588736075907946\n",
      "Iter 6600, loss 0.0002383901592111215\n",
      "Iter 6700, loss 0.00024101213784888387\n",
      "Iter 6800, loss 0.00024375328212045133\n",
      "Iter 6900, loss 0.0002464944263920188\n",
      "Iter 7000, loss 0.0002493547508493066\n",
      "Iter 7100, loss 0.00025233422638848424\n",
      "Iter 7200, loss 0.0002553137019276619\n",
      "Iter 7300, loss 0.0002584123576525599\n",
      "Iter 7400, loss 0.0002616301644593477\n",
      "Iter 7500, loss 0.0002649671514518559\n",
      "Iter 7600, loss 0.00026842328952625394\n",
      "Iter 7700, loss 0.0002719986077863723\n",
      "Iter 7800, loss 0.00027569307712838054\n",
      "Iter 7900, loss 0.0002795067266561091\n",
      "Iter 8000, loss 0.0002834395272657275\n",
      "Iter 8100, loss 0.00028761065914295614\n",
      "Iter 8200, loss 0.00029202012228779495\n",
      "Iter 8300, loss 0.0002965487365145236\n",
      "Iter 8400, loss 0.00030119650182314217\n",
      "Iter 8500, loss 0.00030620177858509123\n",
      "Iter 8600, loss 0.00031132620642893016\n",
      "Iter 8700, loss 0.00031668893643654883\n",
      "Iter 8800, loss 0.00032240914879366755\n",
      "Iter 8900, loss 0.00032848684350028634\n",
      "Iter 9000, loss 0.00033480284037068486\n",
      "Iter 9100, loss 0.0003415954706724733\n",
      "Iter 9200, loss 0.0003486264031380415\n",
      "Iter 9300, loss 0.00035637227119877934\n",
      "Iter 9400, loss 0.00036435641231946647\n",
      "Iter 9500, loss 0.00037293630884960294\n",
      "Iter 9600, loss 0.00038223114097490907\n",
      "Iter 9700, loss 0.00039200251922011375\n",
      "Iter 9800, loss 0.0004024887748528272\n",
      "Iter 9900, loss 0.00041368984966538846\n",
      "Iter 0, loss 0.00031990656862035394\n",
      "Iter 100, loss 0.00032646095496602356\n",
      "Iter 200, loss 0.0003333727945573628\n",
      "Iter 300, loss 0.0003405229654163122\n",
      "Iter 400, loss 0.00034814971149899065\n",
      "Iter 500, loss 0.000356253091013059\n",
      "Iter 600, loss 0.00036483307485468686\n",
      "Iter 700, loss 0.00037400881410576403\n",
      "Iter 800, loss 0.00038389943074434996\n",
      "Iter 900, loss 0.0003943857445847243\n",
      "Iter 1000, loss 0.00040558696491643786\n",
      "Iter 1100, loss 0.000417741306591779\n",
      "Iter 1200, loss 0.00043096792069263756\n",
      "Iter 1300, loss 0.00044550508027896285\n",
      "Iter 1400, loss 0.0004615910293068737\n",
      "Iter 1500, loss 0.0004789874074049294\n",
      "Iter 1600, loss 0.000498289882671088\n",
      "Iter 1700, loss 0.0005198557628318667\n",
      "Iter 1800, loss 0.0005436849314719439\n",
      "Iter 1900, loss 0.0005708495154976845\n",
      "Iter 2000, loss 0.000601349223870784\n",
      "Iter 2100, loss 0.0006361367995850742\n",
      "Iter 2200, loss 0.0006765222642570734\n",
      "Iter 2300, loss 0.0007231004419736564\n",
      "Iter 2400, loss 0.0007784912013448775\n",
      "Iter 2500, loss 0.0008453133050352335\n",
      "Iter 2600, loss 0.0009283285471610725\n",
      "Iter 2700, loss 0.0010363452602177858\n",
      "Iter 2800, loss 0.0011837625643238425\n",
      "Iter 2900, loss 0.0013953244779258966\n",
      "Iter 3000, loss 0.0018262865487486124\n",
      "Iter 3100, loss 0.003304499201476574\n",
      "Iter 0, loss 0.0026889618020504713\n",
      "Iter 0, loss 0.0024000192061066628\n",
      "Iter 0, loss 0.00010108436981681734\n",
      "Iter 100, loss 0.00010144196130568162\n",
      "Iter 200, loss 0.00010179955279454589\n",
      "Iter 300, loss 0.00010227633902104571\n",
      "Iter 400, loss 0.00010263393050990999\n",
      "Iter 500, loss 0.00010299152199877426\n",
      "Iter 600, loss 0.00010334911348763853\n",
      "Iter 700, loss 0.0001037067049765028\n",
      "Iter 800, loss 0.00010406429646536708\n",
      "Iter 900, loss 0.00010442188795423135\n",
      "Iter 1000, loss 0.00010477947944309562\n",
      "Iter 1100, loss 0.00010525626566959545\n",
      "Iter 1200, loss 0.00010561384988250211\n",
      "Iter 1300, loss 0.00010597144137136638\n",
      "Iter 1400, loss 0.00010644822759786621\n",
      "Iter 1500, loss 0.00010680581908673048\n",
      "Iter 1600, loss 0.00010716341057559475\n",
      "Iter 1700, loss 0.00010764019680209458\n",
      "Iter 1800, loss 0.00010799778101500124\n",
      "Iter 1900, loss 0.00010847456724150106\n",
      "Iter 2000, loss 0.00010883215873036534\n",
      "Iter 2100, loss 0.00010930894495686516\n",
      "Iter 2200, loss 0.00010966652916977182\n",
      "Iter 2300, loss 0.00011014331539627165\n",
      "Iter 2400, loss 0.00011050090688513592\n",
      "Iter 2500, loss 0.00011097769311163574\n",
      "Iter 2600, loss 0.00011145447206217796\n",
      "Iter 2700, loss 0.00011181206355104223\n",
      "Iter 2800, loss 0.00011228884250158444\n",
      "Iter 2900, loss 0.00011276562872808427\n",
      "Iter 3000, loss 0.00011324241495458409\n",
      "Iter 3100, loss 0.0001137191939051263\n",
      "Iter 3200, loss 0.00011419598013162613\n",
      "Iter 3300, loss 0.00011467275908216834\n",
      "Iter 3400, loss 0.00011503035057103261\n",
      "Iter 3500, loss 0.00011562632425921038\n",
      "Iter 3600, loss 0.0001161031104857102\n",
      "Iter 3700, loss 0.00011657988943625242\n",
      "Iter 3800, loss 0.00011705666838679463\n",
      "Iter 3900, loss 0.00011753345461329445\n",
      "Iter 4000, loss 0.00011801023356383666\n",
      "Iter 4100, loss 0.00011848701251437888\n",
      "Iter 4200, loss 0.00011908298620255664\n",
      "Iter 4300, loss 0.00011955977242905647\n",
      "Iter 4400, loss 0.00012015574611723423\n",
      "Iter 4500, loss 0.00012063252506777644\n",
      "Iter 4600, loss 0.00012110930401831865\n",
      "Iter 4700, loss 0.00012170527770649642\n",
      "Iter 4800, loss 0.00012230125139467418\n",
      "Iter 4900, loss 0.0001227780303452164\n",
      "Iter 5000, loss 0.00012337400403339416\n",
      "Iter 5100, loss 0.00012396997772157192\n",
      "Iter 5200, loss 0.00012444675667211413\n",
      "Iter 5300, loss 0.0001250427303602919\n",
      "Iter 5400, loss 0.00012563870404846966\n",
      "Iter 5500, loss 0.0001262346631847322\n",
      "Iter 5600, loss 0.00012683063687290996\n",
      "Iter 5700, loss 0.00012742661056108773\n",
      "Iter 5800, loss 0.0001280225842492655\n",
      "Iter 5900, loss 0.00012861855793744326\n",
      "Iter 6000, loss 0.0001292145170737058\n",
      "Iter 6100, loss 0.00012981049076188356\n",
      "Iter 6200, loss 0.00013052565918769687\n",
      "Iter 6300, loss 0.0001311216183239594\n",
      "Iter 6400, loss 0.00013171759201213717\n",
      "Iter 6500, loss 0.00013243274588603526\n",
      "Iter 6600, loss 0.00013302871957421303\n",
      "Iter 6700, loss 0.00013374387344811112\n",
      "Iter 6800, loss 0.00013445904187392443\n",
      "Iter 6900, loss 0.00013505500101018697\n",
      "Iter 7000, loss 0.0001357701694360003\n",
      "Iter 7100, loss 0.00013648532330989838\n",
      "Iter 7200, loss 0.00013720047718379647\n",
      "Iter 7300, loss 0.00013791563105769455\n",
      "Iter 7400, loss 0.00013863079948350787\n",
      "Iter 7500, loss 0.00013934595335740596\n",
      "Iter 7600, loss 0.00014006110723130405\n",
      "Iter 7700, loss 0.00014077626110520214\n",
      "Iter 7800, loss 0.00014161060971673578\n",
      "Iter 7900, loss 0.00014232576359063387\n",
      "Iter 8000, loss 0.0001431601122021675\n",
      "Iter 8100, loss 0.00014399446081370115\n",
      "Iter 8200, loss 0.000144709600135684\n",
      "Iter 8300, loss 0.00014554394874721766\n",
      "Iter 8400, loss 0.0001463782973587513\n",
      "Iter 8500, loss 0.0001472126314183697\n",
      "Iter 8600, loss 0.00014804698002990335\n",
      "Iter 8700, loss 0.00014888131408952177\n",
      "Iter 8800, loss 0.0001497156627010554\n",
      "Iter 8900, loss 0.00015066919149830937\n",
      "Iter 9000, loss 0.0001515035255579278\n",
      "Iter 9100, loss 0.00015245705435518175\n",
      "Iter 9200, loss 0.00015341058315243572\n",
      "Iter 9300, loss 0.00015436411194968969\n",
      "Iter 9400, loss 0.00015531764074694365\n",
      "Iter 9500, loss 0.00015627116954419762\n",
      "Iter 9600, loss 0.00015722469834145159\n",
      "Iter 9700, loss 0.00015817821258679032\n",
      "Iter 9800, loss 0.00015925093612167984\n",
      "Iter 9900, loss 0.00016020445036701858\n",
      "Iter 0, loss 0.00023469554434996098\n",
      "Iter 100, loss 0.00023672162205912173\n",
      "Iter 200, loss 0.00023898606013972312\n",
      "Iter 300, loss 0.0002411313180346042\n",
      "Iter 400, loss 0.00024351492174901068\n",
      "Iter 500, loss 0.00024577934527769685\n",
      "Iter 600, loss 0.00024816294899210334\n",
      "Iter 700, loss 0.0002506657037883997\n",
      "Iter 800, loss 0.0002531684876885265\n",
      "Iter 900, loss 0.0002557904226705432\n",
      "Iter 1000, loss 0.0002585315378382802\n",
      "Iter 1100, loss 0.00026127262390218675\n",
      "Iter 1200, loss 0.0002640137099660933\n",
      "Iter 1300, loss 0.0002669931564014405\n",
      "Iter 1400, loss 0.0002699726028367877\n",
      "Iter 1500, loss 0.00027295202016830444\n",
      "Iter 1600, loss 0.00027616979787126184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1700, loss 0.00027938754647038877\n",
      "Iter 1800, loss 0.0002826052950695157\n",
      "Iter 1900, loss 0.00028606137493625283\n",
      "Iter 2000, loss 0.00028951745480298996\n",
      "Iter 2100, loss 0.0002932118659373373\n",
      "Iter 2200, loss 0.00029690624796785414\n",
      "Iter 2300, loss 0.00030071981018409133\n",
      "Iter 2400, loss 0.0003047717036679387\n",
      "Iter 2500, loss 0.00030882356804795563\n",
      "Iter 2600, loss 0.0003131137927994132\n",
      "Iter 2700, loss 0.0003175231395289302\n",
      "Iter 2800, loss 0.00032217081752605736\n",
      "Iter 2900, loss 0.0003270567976869643\n",
      "Iter 3000, loss 0.00033206192892976105\n",
      "Iter 3100, loss 0.000337305391440168\n",
      "Iter 3200, loss 0.00034278715611435473\n",
      "Iter 3300, loss 0.00034850722295232117\n",
      "Iter 3400, loss 0.00035446559195406735\n",
      "Iter 3500, loss 0.0003607814433053136\n",
      "Iter 3600, loss 0.0003673355677165091\n",
      "Iter 3700, loss 0.00037424711626954377\n",
      "Iter 3800, loss 0.0003815161471720785\n",
      "Iter 3900, loss 0.00038926175329834223\n",
      "Iter 4000, loss 0.00039736481267027557\n",
      "Iter 4100, loss 0.0004058252670802176\n",
      "Iter 4200, loss 0.0004146431456319988\n",
      "Iter 4300, loss 0.0004240567213855684\n",
      "Iter 4400, loss 0.00043418517452664673\n",
      "Iter 4500, loss 0.0004447901446837932\n",
      "Iter 4600, loss 0.0004564673872664571\n",
      "Iter 4700, loss 0.0004690977220889181\n",
      "Iter 4800, loss 0.000482442817883566\n",
      "Iter 4900, loss 0.0004967409186065197\n",
      "Iter 5000, loss 0.0005123494775034487\n",
      "Iter 5100, loss 0.0005290300468914211\n",
      "Iter 5200, loss 0.000547378440387547\n",
      "Iter 5300, loss 0.0005673944251611829\n",
      "Iter 5400, loss 0.0005896736984141171\n",
      "Iter 5500, loss 0.0006140968762338161\n",
      "Iter 5600, loss 0.0006413786904886365\n",
      "Iter 5700, loss 0.00067223358200863\n",
      "Iter 5800, loss 0.0007076143519952893\n",
      "Iter 5900, loss 0.0007475204183720052\n",
      "Iter 6000, loss 0.0007938570925034583\n",
      "Iter 6100, loss 0.0008480527903884649\n",
      "Iter 6200, loss 0.0009116546134464443\n",
      "Iter 6300, loss 0.0009891861118376255\n",
      "Iter 6400, loss 0.0010877889581024647\n",
      "Iter 6500, loss 0.0012162677012383938\n",
      "Iter 6600, loss 0.00139318173751235\n",
      "Iter 6700, loss 0.0016793209360912442\n",
      "Iter 6800, loss 0.002314152894541621\n",
      "Iter 6900, loss 0.004403537139296532\n",
      "Iter 0, loss 0.0002814135223161429\n",
      "Iter 100, loss 0.00028606137493625283\n",
      "Iter 200, loss 0.0002908283786382526\n",
      "Iter 300, loss 0.00029595286468975246\n",
      "Iter 400, loss 0.0003013156820088625\n",
      "Iter 500, loss 0.0003069168305955827\n",
      "Iter 600, loss 0.00031287543242797256\n",
      "Iter 700, loss 0.00031919151660986245\n",
      "Iter 800, loss 0.00032586511224508286\n",
      "Iter 900, loss 0.0003327769518364221\n",
      "Iter 1000, loss 0.0003400462737772614\n",
      "Iter 1100, loss 0.00034791138023138046\n",
      "Iter 1200, loss 0.00035637227119877934\n",
      "Iter 1300, loss 0.00036530973738990724\n",
      "Iter 1400, loss 0.0003748429589904845\n",
      "Iter 1500, loss 0.00038485272671096027\n",
      "Iter 1600, loss 0.0003955773718189448\n",
      "Iter 1700, loss 0.00040713604539632797\n",
      "Iter 1800, loss 0.00041976699139922857\n",
      "Iter 1900, loss 0.00043335105874575675\n",
      "Iter 2000, loss 0.00044800734031014144\n",
      "Iter 2100, loss 0.0004640932602342218\n",
      "Iter 2200, loss 0.0004818470624741167\n",
      "Iter 2300, loss 0.0005015069036744535\n",
      "Iter 2400, loss 0.0005228344234637916\n",
      "Iter 2500, loss 0.000546425289940089\n",
      "Iter 2600, loss 0.0005726366653107107\n",
      "Iter 2700, loss 0.0006018257699906826\n",
      "Iter 2800, loss 0.0006348263123072684\n",
      "Iter 2900, loss 0.00067223358200863\n",
      "Iter 3000, loss 0.0007151191821321845\n",
      "Iter 3100, loss 0.0007651500636711717\n",
      "Iter 3200, loss 0.0008251837571151555\n",
      "Iter 3300, loss 0.0008983152220025659\n",
      "Iter 3400, loss 0.000990377040579915\n",
      "Iter 3500, loss 0.0011152960360050201\n",
      "Iter 3600, loss 0.001291037304326892\n",
      "Iter 3700, loss 0.0016037471359595656\n",
      "Iter 3800, loss 0.0024330795276910067\n",
      "Iter 0, loss 0.0007715824176557362\n",
      "Iter 100, loss 0.0008419782971031964\n",
      "Iter 200, loss 0.0009301149984821677\n",
      "Iter 300, loss 0.0010453957365825772\n",
      "Iter 400, loss 0.0012045992771163583\n",
      "Iter 500, loss 0.001459367573261261\n",
      "Iter 600, loss 0.0020561523269861937\n",
      "Iter 700, loss 0.005769267678260803\n",
      "Iter 0, loss 0.026078062132000923\n",
      "Iter 0, loss 0.00026353701832704246\n",
      "Iter 100, loss 0.00026723151677288115\n",
      "Iter 200, loss 0.00027104519540444016\n",
      "Iter 300, loss 0.00027509720530360937\n",
      "Iter 400, loss 0.00027926836628466845\n",
      "Iter 500, loss 0.0002836778585333377\n",
      "Iter 600, loss 0.00028832571115344763\n",
      "Iter 700, loss 0.00029309268575161695\n",
      "Iter 800, loss 0.0002980979916173965\n",
      "Iter 900, loss 0.00030322244856506586\n",
      "Iter 1000, loss 0.00030858523678034544\n",
      "Iter 1100, loss 0.0003143055073451251\n",
      "Iter 1200, loss 0.00032014489988796413\n",
      "Iter 1300, loss 0.00032634177478030324\n",
      "Iter 1400, loss 0.00033301531220786273\n",
      "Iter 1500, loss 0.0003400462737772614\n",
      "Iter 1600, loss 0.0003475538978818804\n",
      "Iter 1700, loss 0.00035553809721022844\n",
      "Iter 1800, loss 0.0003641180810518563\n",
      "Iter 1900, loss 0.00037305548903532326\n",
      "Iter 2000, loss 0.000382707774406299\n",
      "Iter 2100, loss 0.00039295581518672407\n",
      "Iter 2200, loss 0.00040391870425082743\n",
      "Iter 2300, loss 0.00041571559268049896\n",
      "Iter 2400, loss 0.0004285847535356879\n",
      "Iter 2500, loss 0.00044252615771256387\n",
      "Iter 2600, loss 0.0004573014739435166\n",
      "Iter 2700, loss 0.00047350639943033457\n",
      "Iter 2800, loss 0.0004913791781291366\n",
      "Iter 2900, loss 0.0005108005134388804\n",
      "Iter 3000, loss 0.000532008707523346\n",
      "Iter 3100, loss 0.0005553610390052199\n",
      "Iter 3200, loss 0.0005809764843434095\n",
      "Iter 3300, loss 0.0006096888100728393\n",
      "Iter 3400, loss 0.0006423317245207727\n",
      "Iter 3500, loss 0.0006789048202335835\n",
      "Iter 3600, loss 0.0007207180024124682\n",
      "Iter 3700, loss 0.0007693191873840988\n",
      "Iter 3800, loss 0.0008264940115623176\n",
      "Iter 3900, loss 0.0008955758530646563\n",
      "Iter 4000, loss 0.0009815642843022943\n",
      "Iter 4100, loss 0.0010920758359134197\n",
      "Iter 4200, loss 0.0012397230602800846\n",
      "Iter 4300, loss 0.0014592485968023539\n",
      "Iter 4400, loss 0.001927424455061555\n",
      "Iter 4500, loss 0.0034438606817275286\n",
      "Iter 0, loss 0.00022301571152638644\n",
      "Iter 100, loss 0.00022551853908225894\n",
      "Iter 200, loss 0.000228140561375767\n",
      "Iter 300, loss 0.0002307625545654446\n",
      "Iter 400, loss 0.0002335037279408425\n",
      "Iter 500, loss 0.00023624490131624043\n",
      "Iter 600, loss 0.00023910524032544345\n",
      "Iter 700, loss 0.0002422039397060871\n",
      "Iter 800, loss 0.00024530262453481555\n",
      "Iter 900, loss 0.0002485204895492643\n",
      "Iter 1000, loss 0.00025185750564560294\n",
      "Iter 1100, loss 0.0002553137019276619\n",
      "Iter 1200, loss 0.0002588890492916107\n",
      "Iter 1300, loss 0.0002627027570270002\n",
      "Iter 1400, loss 0.00026663561584427953\n",
      "Iter 1500, loss 0.0002708068350329995\n",
      "Iter 1600, loss 0.00027509720530360937\n",
      "Iter 1700, loss 0.0002796259068418294\n",
      "Iter 1800, loss 0.00028427375946193933\n",
      "Iter 1900, loss 0.0002892790944315493\n",
      "Iter 2000, loss 0.0002944036095868796\n",
      "Iter 2100, loss 0.00029976642690598965\n",
      "Iter 2200, loss 0.0003054867556784302\n",
      "Iter 2300, loss 0.00031156453769654036\n",
      "Iter 2400, loss 0.0003178806509822607\n",
      "Iter 2500, loss 0.00032455421751365066\n",
      "Iter 2600, loss 0.000331704446580261\n",
      "Iter 2700, loss 0.00033909291960299015\n",
      "Iter 2800, loss 0.0003469580551609397\n",
      "Iter 2900, loss 0.00035529976594261825\n",
      "Iter 3000, loss 0.0003641180810518563\n",
      "Iter 3100, loss 0.00037353215157054365\n",
      "Iter 3200, loss 0.00038366109947673976\n",
      "Iter 3300, loss 0.0003946240758523345\n",
      "Iter 3400, loss 0.00040642108069732785\n",
      "Iter 3500, loss 0.0004190520558040589\n",
      "Iter 3600, loss 0.00043275527423247695\n",
      "Iter 3700, loss 0.00044741155579686165\n",
      "Iter 3800, loss 0.0004634975048247725\n",
      "Iter 3900, loss 0.0004814896092284471\n",
      "Iter 4000, loss 0.0005011494504287839\n",
      "Iter 4100, loss 0.0005227153305895627\n",
      "Iter 4200, loss 0.0005465444410219789\n",
      "Iter 4300, loss 0.0005738280597142875\n",
      "Iter 4400, loss 0.000604565953835845\n",
      "Iter 4500, loss 0.0006394725642167032\n",
      "Iter 4600, loss 0.0006792622152715921\n",
      "Iter 4700, loss 0.0007256020326167345\n",
      "Iter 4800, loss 0.0007795632118359208\n",
      "Iter 4900, loss 0.0008434075862169266\n",
      "Iter 5000, loss 0.0009222545195370913\n",
      "Iter 5100, loss 0.0010252702049911022\n",
      "Iter 5200, loss 0.0011642351746559143\n",
      "Iter 5300, loss 0.0013711584033444524\n",
      "Iter 5400, loss 0.0017915404168888927\n",
      "Iter 5500, loss 0.0030083658639341593\n",
      "Iter 0, loss 0.001493173069320619\n",
      "Iter 100, loss 0.002108614193275571\n",
      "Iter 200, loss 0.0044981250539422035\n",
      "Iter 0, loss 0.0005759726045653224\n",
      "Iter 100, loss 0.0006093314150348306\n",
      "Iter 200, loss 0.0006472161621786654\n",
      "Iter 300, loss 0.0006920088781043887\n",
      "Iter 400, loss 0.0007453762227669358\n",
      "Iter 500, loss 0.0008092227508313954\n",
      "Iter 600, loss 0.0008883106056600809\n",
      "Iter 700, loss 0.000990615226328373\n",
      "Iter 800, loss 0.0011310139670968056\n",
      "Iter 900, loss 0.001347825163975358\n",
      "Iter 1000, loss 0.0017952292691916227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1100, loss 0.0033033110667020082\n",
      "Iter 0, loss 0.0012309125158935785\n",
      "Iter 100, loss 0.0014422263484448195\n",
      "Iter 200, loss 0.0017889224691316485\n",
      "Iter 300, loss 0.002719872398301959\n",
      "Iter 0, loss 0.0024103655014187098\n",
      "Iter 0, loss 0.0003779412363655865\n",
      "Iter 100, loss 0.0003885467885993421\n",
      "Iter 200, loss 0.0003999863693024963\n",
      "Iter 300, loss 0.00041237910045310855\n",
      "Iter 400, loss 0.00042572495294734836\n",
      "Iter 500, loss 0.000440262199845165\n",
      "Iter 600, loss 0.00045599075383506715\n",
      "Iter 700, loss 0.00047302976599894464\n",
      "Iter 800, loss 0.0004919749335385859\n",
      "Iter 900, loss 0.0005133026279509068\n",
      "Iter 1000, loss 0.0005368936690501869\n",
      "Iter 1100, loss 0.000563224486541003\n",
      "Iter 1200, loss 0.0005927712772972882\n",
      "Iter 1300, loss 0.0006260104128159583\n",
      "Iter 1400, loss 0.0006644901586696506\n",
      "Iter 1500, loss 0.0007095203618519008\n",
      "Iter 1600, loss 0.0007640779949724674\n",
      "Iter 1700, loss 0.0008305437513627112\n",
      "Iter 1800, loss 0.0009127265075221658\n",
      "Iter 1900, loss 0.001019673072732985\n",
      "Iter 2000, loss 0.001165664056316018\n",
      "Iter 2100, loss 0.0013785392511636019\n",
      "Iter 2200, loss 0.0018156962469220161\n",
      "Iter 2300, loss 0.0033301631920039654\n",
      "Iter 0, loss 0.0002307625545654446\n",
      "Iter 100, loss 0.0002337421028641984\n",
      "Iter 200, loss 0.00023684080224484205\n",
      "Iter 300, loss 0.0002401778765488416\n",
      "Iter 400, loss 0.00024351492174901068\n",
      "Iter 500, loss 0.0002470903273206204\n",
      "Iter 600, loss 0.0002506657037883997\n",
      "Iter 700, loss 0.0002544794406276196\n",
      "Iter 800, loss 0.0002584123576525599\n",
      "Iter 900, loss 0.00026258357684127986\n",
      "Iter 1000, loss 0.0002669931564014405\n",
      "Iter 1100, loss 0.000271521887043491\n",
      "Iter 1200, loss 0.00027616979787126184\n",
      "Iter 1300, loss 0.0002811751910485327\n",
      "Iter 1400, loss 0.0002862997353076935\n",
      "Iter 1500, loss 0.0002917817619163543\n",
      "Iter 1600, loss 0.0002975021197926253\n",
      "Iter 1700, loss 0.00030357998912222683\n",
      "Iter 1800, loss 0.00031013446277938783\n",
      "Iter 1900, loss 0.00031716562807559967\n",
      "Iter 2000, loss 0.00032455421751365066\n",
      "Iter 2100, loss 0.00033241944038309157\n",
      "Iter 2200, loss 0.0003407612966839224\n",
      "Iter 2300, loss 0.0003496989083942026\n",
      "Iter 2400, loss 0.0003592322755139321\n",
      "Iter 2500, loss 0.0003694805200211704\n",
      "Iter 2600, loss 0.00038044367101974785\n",
      "Iter 2700, loss 0.0003923600015696138\n",
      "Iter 2800, loss 0.0004051103023812175\n",
      "Iter 2900, loss 0.0004191712068859488\n",
      "Iter 3000, loss 0.00043442347669042647\n",
      "Iter 3100, loss 0.00045098623377270997\n",
      "Iter 3200, loss 0.0004694551753345877\n",
      "Iter 3300, loss 0.000490068516228348\n",
      "Iter 3400, loss 0.0005133026279509068\n",
      "Iter 3500, loss 0.0005393957253545523\n",
      "Iter 3600, loss 0.0005683475756086409\n",
      "Iter 3700, loss 0.0006017066189087927\n",
      "Iter 3800, loss 0.0006404255982488394\n",
      "Iter 3900, loss 0.0006854568491689861\n",
      "Iter 4000, loss 0.0007383481133729219\n",
      "Iter 4100, loss 0.0008043391280807555\n",
      "Iter 4200, loss 0.0008871195605024695\n",
      "Iter 4300, loss 0.0009957361035048962\n",
      "Iter 4400, loss 0.0011519708205014467\n",
      "Iter 4500, loss 0.0014240134041756392\n",
      "Iter 4600, loss 0.002125387080013752\n",
      "Iter 4700, loss 0.03027377463877201\n",
      "Iter 0, loss 0.000950004265177995\n",
      "Iter 100, loss 0.001064687268808484\n",
      "Iter 200, loss 0.0012306743301451206\n",
      "Iter 300, loss 0.0015416176756843925\n",
      "Iter 400, loss 0.0022882248740643263\n",
      "Iter 500, loss 0.1322655826807022\n",
      "Iter 0, loss 0.00016032364510465413\n",
      "Iter 100, loss 0.00016127715934999287\n",
      "Iter 200, loss 0.00016223068814724684\n",
      "Iter 300, loss 0.00016330339713022113\n",
      "Iter 400, loss 0.00016425691137555987\n",
      "Iter 500, loss 0.00016532962035853416\n",
      "Iter 600, loss 0.00016640232934150845\n",
      "Iter 700, loss 0.0001674750237725675\n",
      "Iter 800, loss 0.0001685477327555418\n",
      "Iter 900, loss 0.00016962042718660086\n",
      "Iter 1000, loss 0.00017069313616957515\n",
      "Iter 1100, loss 0.00017188502533826977\n",
      "Iter 1200, loss 0.00017307691450696439\n",
      "Iter 1300, loss 0.00017426878912374377\n",
      "Iter 1400, loss 0.0001754606782924384\n",
      "Iter 1500, loss 0.000176652567461133\n",
      "Iter 1600, loss 0.00017796363681554794\n",
      "Iter 1700, loss 0.00017915551143232733\n",
      "Iter 1800, loss 0.00018046658078674227\n",
      "Iter 1900, loss 0.0001817776501411572\n",
      "Iter 2000, loss 0.00018320789968129247\n",
      "Iter 2100, loss 0.00018451895448379219\n",
      "Iter 2200, loss 0.00018594920402392745\n",
      "Iter 2300, loss 0.00018737945356406271\n",
      "Iter 2400, loss 0.00018880968855228275\n",
      "Iter 2500, loss 0.0001902399235405028\n",
      "Iter 2600, loss 0.00019178935326635838\n",
      "Iter 2700, loss 0.00019333878299221396\n",
      "Iter 2800, loss 0.00019488819816615433\n",
      "Iter 2900, loss 0.00019643761334009469\n",
      "Iter 3000, loss 0.00019810620869975537\n",
      "Iter 3100, loss 0.0001998939987970516\n",
      "Iter 3200, loss 0.0002015625941567123\n",
      "Iter 3300, loss 0.00020346954988781363\n",
      "Iter 3400, loss 0.00020525732543319464\n",
      "Iter 3500, loss 0.00020716428116429597\n",
      "Iter 3600, loss 0.0002090712368953973\n",
      "Iter 3700, loss 0.00021109737281221896\n",
      "Iter 3800, loss 0.0002131234941771254\n",
      "Iter 3900, loss 0.00021526881027966738\n",
      "Iter 4000, loss 0.0002172949316445738\n",
      "Iter 4100, loss 0.0002195594133809209\n",
      "Iter 4200, loss 0.0002217047003796324\n",
      "Iter 4300, loss 0.0002240883477497846\n",
      "Iter 4400, loss 0.00022635281493421644\n",
      "Iter 4500, loss 0.00022885564249008894\n",
      "Iter 4600, loss 0.00023135847004596144\n",
      "Iter 4700, loss 0.0002338612830499187\n",
      "Iter 4800, loss 0.0002366024418734014\n",
      "Iter 4900, loss 0.00023934361524879932\n",
      "Iter 5000, loss 0.0002420847595203668\n",
      "Iter 5100, loss 0.0002450642641633749\n",
      "Iter 5200, loss 0.000248043768806383\n",
      "Iter 5300, loss 0.000251142424531281\n",
      "Iter 5400, loss 0.0002543602604418993\n",
      "Iter 5500, loss 0.00025769727653823793\n",
      "Iter 5600, loss 0.0002611534437164664\n",
      "Iter 5700, loss 0.0002646096108946949\n",
      "Iter 5800, loss 0.0002683041093405336\n",
      "Iter 5900, loss 0.00027211778797209263\n",
      "Iter 6000, loss 0.0002759314374998212\n",
      "Iter 6100, loss 0.00027998341829515994\n",
      "Iter 6200, loss 0.000284154579276219\n",
      "Iter 6300, loss 0.0002885640424210578\n",
      "Iter 6400, loss 0.00029309268575161695\n",
      "Iter 6500, loss 0.00029797881143167615\n",
      "Iter 6600, loss 0.00030286493711173534\n",
      "Iter 6700, loss 0.0003081085451412946\n",
      "Iter 6800, loss 0.0003135904553346336\n",
      "Iter 6900, loss 0.00031931069679558277\n",
      "Iter 7000, loss 0.0003252692404203117\n",
      "Iter 7100, loss 0.00033146608620882034\n",
      "Iter 7200, loss 0.00033802041434682906\n",
      "Iter 7300, loss 0.00034505134681239724\n",
      "Iter 7400, loss 0.00035232058144174516\n",
      "Iter 7500, loss 0.00036006642039865255\n",
      "Iter 7600, loss 0.0003682888636831194\n",
      "Iter 7700, loss 0.0003771070914808661\n",
      "Iter 7800, loss 0.00038628268521279097\n",
      "Iter 7900, loss 0.00039593485416844487\n",
      "Iter 8000, loss 0.00040642108069732785\n",
      "Iter 8100, loss 0.000417741306591779\n",
      "Iter 8200, loss 0.0004297763225622475\n",
      "Iter 8300, loss 0.00044264530879445374\n",
      "Iter 8400, loss 0.0004567056894302368\n",
      "Iter 8500, loss 0.00047183825518004596\n",
      "Iter 8600, loss 0.0004885195521637797\n",
      "Iter 8700, loss 0.0005071069463156164\n",
      "Iter 8800, loss 0.0005273620481602848\n",
      "Iter 8900, loss 0.0005497612874023616\n",
      "Iter 9000, loss 0.0005743046058341861\n",
      "Iter 9100, loss 0.0006014683749526739\n",
      "Iter 9200, loss 0.000631848000921309\n",
      "Iter 9300, loss 0.0006659197388216853\n",
      "Iter 9400, loss 0.0007049936102703214\n",
      "Iter 9500, loss 0.0007509748684242368\n",
      "Iter 9600, loss 0.0008046964649111032\n",
      "Iter 9700, loss 0.0008685392094776034\n",
      "Iter 9800, loss 0.0009467886411584914\n",
      "Iter 9900, loss 0.0010469438275322318\n",
      "Iter 0, loss 0.0002829628065228462\n",
      "Iter 100, loss 0.00028772983932867646\n",
      "Iter 200, loss 0.00029273517429828644\n",
      "Iter 300, loss 0.00029797881143167615\n",
      "Iter 400, loss 0.0003034608089365065\n",
      "Iter 500, loss 0.0003093002596870065\n",
      "Iter 600, loss 0.0003152588615193963\n",
      "Iter 700, loss 0.0003216941258870065\n",
      "Iter 800, loss 0.000328367663314566\n",
      "Iter 900, loss 0.00033539868309162557\n",
      "Iter 1000, loss 0.0003430254873819649\n",
      "Iter 1100, loss 0.00035124807618558407\n",
      "Iter 1200, loss 0.0003601856005843729\n",
      "Iter 1300, loss 0.0003695997002068907\n",
      "Iter 1400, loss 0.0003797286772169173\n",
      "Iter 1500, loss 0.0003904534096363932\n",
      "Iter 1600, loss 0.000402250443585217\n",
      "Iter 1700, loss 0.00041500062798149884\n",
      "Iter 1800, loss 0.000428942235885188\n",
      "Iter 1900, loss 0.00044383687782101333\n",
      "Iter 2000, loss 0.00046004203613847494\n",
      "Iter 2100, loss 0.00047803416964598\n",
      "Iter 2200, loss 0.0004980515805073082\n",
      "Iter 2300, loss 0.0005203323671594262\n",
      "Iter 2400, loss 0.0005447572330012918\n",
      "Iter 2500, loss 0.0005721600609831512\n",
      "Iter 2600, loss 0.0006032554083503783\n",
      "Iter 2700, loss 0.0006387577159330249\n",
      "Iter 2800, loss 0.0006797387031838298\n",
      "Iter 2900, loss 0.0007277462864294648\n",
      "Iter 3000, loss 0.0007857572636567056\n",
      "Iter 3100, loss 0.0008557948167435825\n",
      "Iter 3200, loss 0.0009420248097740114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3300, loss 0.0010519453790038824\n",
      "Iter 3400, loss 0.0012001938885077834\n",
      "Iter 3500, loss 0.0014211564557626843\n",
      "Iter 3600, loss 0.0018542492762207985\n",
      "Iter 3700, loss 0.0033001031260937452\n",
      "Iter 0, loss 0.0010870745172724128\n",
      "Iter 100, loss 0.0012687736889347434\n",
      "Iter 200, loss 0.0015975582646206021\n",
      "Iter 300, loss 0.0025548457633703947\n",
      "Iter 0, loss 0.0003002431185450405\n",
      "Iter 100, loss 0.0003067976504098624\n",
      "Iter 200, loss 0.0003137096355203539\n",
      "Iter 300, loss 0.00032085992279462516\n",
      "Iter 400, loss 0.00032860602368600667\n",
      "Iter 500, loss 0.0003367095487192273\n",
      "Iter 600, loss 0.00034540885826572776\n",
      "Iter 700, loss 0.00035470392322167754\n",
      "Iter 800, loss 0.00036459474358707666\n",
      "Iter 900, loss 0.00037520044133998454\n",
      "Iter 1000, loss 0.000386640167562291\n",
      "Iter 1100, loss 0.0003987947420682758\n",
      "Iter 1200, loss 0.0004120216181036085\n",
      "Iter 1300, loss 0.0004262015863787383\n",
      "Iter 1400, loss 0.0004413345886860043\n",
      "Iter 1500, loss 0.0004580163804348558\n",
      "Iter 1600, loss 0.0004764852055814117\n",
      "Iter 1700, loss 0.000497336674015969\n",
      "Iter 1800, loss 0.0005206898204050958\n",
      "Iter 1900, loss 0.0005466635921038687\n",
      "Iter 2000, loss 0.0005768066039308906\n",
      "Iter 2100, loss 0.0006119524477981031\n",
      "Iter 2200, loss 0.0006526962388306856\n",
      "Iter 2300, loss 0.0007008241955190897\n",
      "Iter 2400, loss 0.0007581220706924796\n",
      "Iter 2500, loss 0.0008266131044365466\n",
      "Iter 2600, loss 0.000912369170691818\n",
      "Iter 2700, loss 0.0010247938334941864\n",
      "Iter 2800, loss 0.0011834054021164775\n",
      "Iter 2900, loss 0.0014641289599239826\n",
      "Iter 3000, loss 0.0022977397311478853\n",
      "Iter 0, loss 0.0007568117580376565\n",
      "Iter 100, loss 0.0008205384365282953\n",
      "Iter 200, loss 0.0008981961291283369\n",
      "Iter 300, loss 0.0009974034037441015\n",
      "Iter 400, loss 0.0011333954753354192\n",
      "Iter 500, loss 0.0013393727131187916\n",
      "Iter 600, loss 0.0017247814685106277\n",
      "Iter 700, loss 0.0030069397762417793\n",
      "Iter 0, loss 0.0002571013756096363\n",
      "Iter 100, loss 0.00026067672297358513\n",
      "Iter 200, loss 0.0002643712505232543\n",
      "Iter 300, loss 0.00026806574896909297\n",
      "Iter 400, loss 0.0002719986077863723\n",
      "Iter 500, loss 0.0002760506176855415\n",
      "Iter 600, loss 0.0002803409588523209\n",
      "Iter 700, loss 0.0002847504511009902\n",
      "Iter 800, loss 0.00028939827461726964\n",
      "Iter 900, loss 0.00029416524921543896\n",
      "Iter 1000, loss 0.0002992897352669388\n",
      "Iter 1100, loss 0.0003045333724003285\n",
      "Iter 1200, loss 0.00031001531169749796\n",
      "Iter 1300, loss 0.00031585473334416747\n",
      "Iter 1400, loss 0.0003219324571546167\n",
      "Iter 1500, loss 0.000328367663314566\n",
      "Iter 1600, loss 0.0003351603518240154\n",
      "Iter 1700, loss 0.00034231049357913435\n",
      "Iter 1800, loss 0.00034981805947609246\n",
      "Iter 1900, loss 0.00035768310772255063\n",
      "Iter 2000, loss 0.000366263062460348\n",
      "Iter 2100, loss 0.00037543877260759473\n",
      "Iter 2200, loss 0.00038521020906046033\n",
      "Iter 2300, loss 0.00039569655200466514\n",
      "Iter 2400, loss 0.0004068977141287178\n",
      "Iter 2500, loss 0.0004191712068859488\n",
      "Iter 2600, loss 0.0004326361231505871\n",
      "Iter 2700, loss 0.0004471732536330819\n",
      "Iter 2800, loss 0.00046302087139338255\n",
      "Iter 2900, loss 0.0004804172203876078\n",
      "Iter 3000, loss 0.0004994813934899867\n",
      "Iter 3100, loss 0.0005203323671594262\n",
      "Iter 3200, loss 0.0005435658385977149\n",
      "Iter 3300, loss 0.0005696581210941076\n",
      "Iter 3400, loss 0.0005989664932712913\n",
      "Iter 3500, loss 0.0006317288498394191\n",
      "Iter 3600, loss 0.0006687788409180939\n",
      "Iter 3700, loss 0.0007124984404072165\n",
      "Iter 3800, loss 0.0007639588438905776\n",
      "Iter 3900, loss 0.0008249455713666975\n",
      "Iter 4000, loss 0.000897957943379879\n",
      "Iter 4100, loss 0.0009919252479448915\n",
      "Iter 4200, loss 0.0011177966371178627\n",
      "Iter 4300, loss 0.0012997282901778817\n",
      "Iter 4400, loss 0.001622075797058642\n",
      "Iter 4500, loss 0.0025324912276118994\n",
      "Iter 0, loss 0.00032586511224508286\n",
      "Iter 100, loss 0.0003332536434754729\n",
      "Iter 200, loss 0.0003409996279515326\n",
      "Iter 300, loss 0.0003492222458589822\n",
      "Iter 400, loss 0.0003580405900720507\n",
      "Iter 500, loss 0.0003673355677165091\n",
      "Iter 600, loss 0.00037746457383036613\n",
      "Iter 700, loss 0.0003883084573317319\n",
      "Iter 800, loss 0.0004001055203843862\n",
      "Iter 900, loss 0.0004128557338844985\n",
      "Iter 1000, loss 0.0004266782198101282\n",
      "Iter 1100, loss 0.0004418112221173942\n",
      "Iter 1200, loss 0.00045849301386624575\n",
      "Iter 1300, loss 0.0004768426588270813\n",
      "Iter 1400, loss 0.0004972175229340792\n",
      "Iter 1500, loss 0.0005197366117499769\n",
      "Iter 1600, loss 0.0005449955351650715\n",
      "Iter 1700, loss 0.0005734706646762788\n",
      "Iter 1800, loss 0.0006059955921955407\n",
      "Iter 1900, loss 0.0006445952458307147\n",
      "Iter 2000, loss 0.0006899837171658874\n",
      "Iter 2100, loss 0.0007443041540682316\n",
      "Iter 2200, loss 0.0008107712492346764\n",
      "Iter 2300, loss 0.0008934320067055523\n",
      "Iter 2400, loss 0.0009996660519391298\n",
      "Iter 2500, loss 0.0011391110019758344\n",
      "Iter 2600, loss 0.0013465156080201268\n",
      "Iter 2700, loss 0.001697053201496601\n",
      "Iter 2800, loss 0.0027514954563230276\n",
      "Iter 0, loss 0.0002653246629051864\n",
      "Iter 100, loss 0.0002694958820939064\n",
      "Iter 200, loss 0.00027378625236451626\n",
      "Iter 300, loss 0.0002783149539027363\n",
      "Iter 400, loss 0.00028308198670856655\n",
      "Iter 500, loss 0.00028796817059628665\n",
      "Iter 600, loss 0.00029309268575161695\n",
      "Iter 700, loss 0.00029845553217455745\n",
      "Iter 800, loss 0.00030393750057555735\n",
      "Iter 900, loss 0.00030989613151177764\n",
      "Iter 1000, loss 0.000316212244797498\n",
      "Iter 1100, loss 0.00032276666024699807\n",
      "Iter 1200, loss 0.0003296785580459982\n",
      "Iter 1300, loss 0.00033706706017255783\n",
      "Iter 1400, loss 0.0003449321957305074\n",
      "Iter 1500, loss 0.0003532739356160164\n",
      "Iter 1600, loss 0.0003620922507252544\n",
      "Iter 1700, loss 0.00037150635034777224\n",
      "Iter 1800, loss 0.0003815161471720785\n",
      "Iter 1900, loss 0.0003922408213838935\n",
      "Iter 2000, loss 0.00040391870425082743\n",
      "Iter 2100, loss 0.00041654970846138895\n",
      "Iter 2200, loss 0.0004300146538298577\n",
      "Iter 2300, loss 0.00044467096449807286\n",
      "Iter 2400, loss 0.0004605186404660344\n",
      "Iter 2500, loss 0.00047779586748220026\n",
      "Iter 2600, loss 0.0004967409186065197\n",
      "Iter 2700, loss 0.0005175919504836202\n",
      "Iter 2800, loss 0.0005407063290476799\n",
      "Iter 2900, loss 0.0005664412747137249\n",
      "Iter 3000, loss 0.0005955114611424506\n",
      "Iter 3100, loss 0.0006283930852077901\n",
      "Iter 3200, loss 0.0006659197388216853\n",
      "Iter 3300, loss 0.0007089247228577733\n",
      "Iter 3400, loss 0.0007588367443531752\n",
      "Iter 3500, loss 0.0008197046699933708\n",
      "Iter 3600, loss 0.0008950994815677404\n",
      "Iter 3700, loss 0.0009927588980644941\n",
      "Iter 3800, loss 0.0011235122801735997\n",
      "Iter 3900, loss 0.0013217531377449632\n",
      "Iter 4000, loss 0.0017298986203968525\n",
      "Iter 4100, loss 0.0028347091283649206\n",
      "Iter 0, loss 0.00031406714697368443\n",
      "Iter 100, loss 0.00032014489988796413\n",
      "Iter 200, loss 0.00032658010604791343\n",
      "Iter 300, loss 0.0003333727945573628\n",
      "Iter 400, loss 0.0003404037852305919\n",
      "Iter 500, loss 0.00034791138023138046\n",
      "Iter 600, loss 0.00035589560866355896\n",
      "Iter 700, loss 0.00036435641231946647\n",
      "Iter 800, loss 0.00037317464011721313\n",
      "Iter 900, loss 0.000382707774406299\n",
      "Iter 1000, loss 0.00039283663500100374\n",
      "Iter 1100, loss 0.00040391870425082743\n",
      "Iter 1200, loss 0.0004158347437623888\n",
      "Iter 1300, loss 0.0004285847535356879\n",
      "Iter 1400, loss 0.00044228785554878414\n",
      "Iter 1500, loss 0.0004570631426759064\n",
      "Iter 1600, loss 0.0004732680681627244\n",
      "Iter 1700, loss 0.0004909025738015771\n",
      "Iter 1800, loss 0.0005103239673189819\n",
      "Iter 1900, loss 0.0005322470096871257\n",
      "Iter 2000, loss 0.0005569098866544664\n",
      "Iter 2100, loss 0.0005845506675541401\n",
      "Iter 2200, loss 0.0006152882124297321\n",
      "Iter 2300, loss 0.0006501944735646248\n",
      "Iter 2400, loss 0.0006906984490342438\n",
      "Iter 2500, loss 0.0007379907765425742\n",
      "Iter 2600, loss 0.0007937379996292293\n",
      "Iter 2700, loss 0.0008627030183561146\n",
      "Iter 2800, loss 0.0009467886411584914\n",
      "Iter 2900, loss 0.00105408881790936\n",
      "Iter 3000, loss 0.0012025751639157534\n",
      "Iter 3100, loss 0.0014255610294640064\n",
      "Iter 3200, loss 0.0018552012043073773\n",
      "Iter 3300, loss 0.003387310542166233\n",
      "Iter 0, loss 0.0003734129713848233\n",
      "Iter 100, loss 0.0003830652858596295\n",
      "Iter 200, loss 0.000393432448618114\n",
      "Iter 300, loss 0.00040463366894982755\n",
      "Iter 400, loss 0.0004170263418927789\n",
      "Iter 500, loss 0.0004303721070755273\n",
      "Iter 600, loss 0.0004450284468475729\n",
      "Iter 700, loss 0.000460876093711704\n",
      "Iter 800, loss 0.0004782725009135902\n",
      "Iter 900, loss 0.0004974558250978589\n",
      "Iter 1000, loss 0.0005185451591387391\n",
      "Iter 1100, loss 0.0005421360838226974\n",
      "Iter 1200, loss 0.0005688241217285395\n",
      "Iter 1300, loss 0.0005990855861455202\n",
      "Iter 1400, loss 0.0006342306733131409\n",
      "Iter 1500, loss 0.0006748544401489198\n",
      "Iter 1600, loss 0.0007217901293188334\n",
      "Iter 1700, loss 0.0007773000397719443\n",
      "Iter 1800, loss 0.0008435266790911555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1900, loss 0.000925112864933908\n",
      "Iter 2000, loss 0.0010318199638277292\n",
      "Iter 2100, loss 0.0011739989276975393\n",
      "Iter 2200, loss 0.0013823487097397447\n",
      "Iter 2300, loss 0.0017848765710368752\n",
      "Iter 2400, loss 0.00304758595302701\n",
      "Iter 0, loss 0.00040880427695810795\n",
      "Iter 100, loss 0.0004203628050163388\n",
      "Iter 200, loss 0.00043275527423247695\n",
      "Iter 300, loss 0.00044610086479224265\n",
      "Iter 400, loss 0.00046039948938414454\n",
      "Iter 500, loss 0.0004757702990900725\n",
      "Iter 600, loss 0.0004924515378661454\n",
      "Iter 700, loss 0.0005104430601932108\n",
      "Iter 800, loss 0.0005302215577103198\n",
      "Iter 900, loss 0.0005521441926248372\n",
      "Iter 1000, loss 0.000576449150685221\n",
      "Iter 1100, loss 0.0006042085005901754\n",
      "Iter 1200, loss 0.0006354220095090568\n",
      "Iter 1300, loss 0.000671042304020375\n",
      "Iter 1400, loss 0.0007115454645827413\n",
      "Iter 1500, loss 0.0007591941393911839\n",
      "Iter 1600, loss 0.000816250394564122\n",
      "Iter 1700, loss 0.0008844992844387889\n",
      "Iter 1800, loss 0.0009666775586083531\n",
      "Iter 1900, loss 0.0010714748641476035\n",
      "Iter 2000, loss 0.001210433547385037\n",
      "Iter 2100, loss 0.0014092524070292711\n",
      "Iter 2200, loss 0.0017603629967197776\n",
      "Iter 2300, loss 0.0027287888806313276\n",
      "Iter 0, loss 0.0008364992681890726\n",
      "Iter 100, loss 0.000920825288631022\n",
      "Iter 200, loss 0.001032296335324645\n",
      "Iter 300, loss 0.0011893587652593851\n",
      "Iter 400, loss 0.0014467497821897268\n",
      "Iter 500, loss 0.0020768519025295973\n",
      "Iter 600, loss 0.005281068850308657\n",
      "Iter 0, loss 0.0008138681878335774\n",
      "Iter 100, loss 0.0008909308817237616\n",
      "Iter 200, loss 0.0009918061550706625\n",
      "Iter 300, loss 0.001128513365983963\n",
      "Iter 400, loss 0.0013247294118627906\n",
      "Iter 500, loss 0.0016680150292813778\n",
      "Iter 600, loss 0.0027462646830826998\n",
      "Iter 0, loss 0.0012748456792905927\n",
      "Iter 100, loss 0.0015870844945311546\n",
      "Iter 200, loss 0.002516319742426276\n",
      "Iter 0, loss 8.093983342405409e-05\n",
      "Iter 100, loss 8.11782301752828e-05\n",
      "Iter 200, loss 8.141662692651153e-05\n",
      "Iter 300, loss 8.153582894010469e-05\n",
      "Iter 400, loss 8.177422569133341e-05\n",
      "Iter 500, loss 8.189342770492658e-05\n",
      "Iter 600, loss 8.21318244561553e-05\n",
      "Iter 700, loss 8.237022848334163e-05\n",
      "Iter 800, loss 8.248942322097719e-05\n",
      "Iter 900, loss 8.272782724816352e-05\n",
      "Iter 1000, loss 8.296622399939224e-05\n",
      "Iter 1100, loss 8.320462075062096e-05\n",
      "Iter 1200, loss 8.332382276421413e-05\n",
      "Iter 1300, loss 8.356221951544285e-05\n",
      "Iter 1400, loss 8.380061626667157e-05\n",
      "Iter 1500, loss 8.40390202938579e-05\n",
      "Iter 1600, loss 8.415821503149346e-05\n",
      "Iter 1700, loss 8.439661905867979e-05\n",
      "Iter 1800, loss 8.463501580990851e-05\n",
      "Iter 1900, loss 8.487341256113723e-05\n",
      "Iter 2000, loss 8.49926145747304e-05\n",
      "Iter 2100, loss 8.523101132595912e-05\n",
      "Iter 2200, loss 8.546940807718784e-05\n",
      "Iter 2300, loss 8.570780482841656e-05\n",
      "Iter 2400, loss 8.594620157964528e-05\n",
      "Iter 2500, loss 8.618460560683161e-05\n",
      "Iter 2600, loss 8.630380034446716e-05\n",
      "Iter 2700, loss 8.654219709569588e-05\n",
      "Iter 2800, loss 8.67805938469246e-05\n",
      "Iter 2900, loss 8.701899787411094e-05\n",
      "Iter 3000, loss 8.725739462533966e-05\n",
      "Iter 3100, loss 8.749579137656838e-05\n",
      "Iter 3200, loss 8.77341881277971e-05\n",
      "Iter 3300, loss 8.797258487902582e-05\n",
      "Iter 3400, loss 8.821098163025454e-05\n",
      "Iter 3500, loss 8.844937838148326e-05\n",
      "Iter 3600, loss 8.868777513271198e-05\n",
      "Iter 3700, loss 8.892617915989831e-05\n",
      "Iter 3800, loss 8.916457591112703e-05\n",
      "Iter 3900, loss 8.940297266235575e-05\n",
      "Iter 4000, loss 8.964136941358447e-05\n",
      "Iter 4100, loss 8.987976616481319e-05\n",
      "Iter 4200, loss 9.011816291604191e-05\n",
      "Iter 4300, loss 9.035655966727063e-05\n",
      "Iter 4400, loss 9.059495641849935e-05\n",
      "Iter 4500, loss 9.083335316972807e-05\n",
      "Iter 4600, loss 9.119095193454996e-05\n",
      "Iter 4700, loss 9.142934868577868e-05\n",
      "Iter 4800, loss 9.16677454370074e-05\n",
      "Iter 4900, loss 9.190614218823612e-05\n",
      "Iter 5000, loss 9.214453893946484e-05\n",
      "Iter 5100, loss 9.250213042832911e-05\n",
      "Iter 5200, loss 9.274052717955783e-05\n",
      "Iter 5300, loss 9.297892393078655e-05\n",
      "Iter 5400, loss 9.321732068201527e-05\n",
      "Iter 5500, loss 9.345571743324399e-05\n",
      "Iter 5600, loss 9.381330892210826e-05\n",
      "Iter 5700, loss 9.405170567333698e-05\n",
      "Iter 5800, loss 9.42901024245657e-05\n",
      "Iter 5900, loss 9.464769391342998e-05\n",
      "Iter 6000, loss 9.48860906646587e-05\n",
      "Iter 6100, loss 9.512448741588742e-05\n",
      "Iter 6200, loss 9.548207890475169e-05\n",
      "Iter 6300, loss 9.572047565598041e-05\n",
      "Iter 6400, loss 9.595887240720913e-05\n",
      "Iter 6500, loss 9.63164638960734e-05\n",
      "Iter 6600, loss 9.655486064730212e-05\n",
      "Iter 6700, loss 9.691245941212401e-05\n",
      "Iter 6800, loss 9.715084888739511e-05\n",
      "Iter 6900, loss 9.7508447652217e-05\n",
      "Iter 7000, loss 9.77468371274881e-05\n",
      "Iter 7100, loss 9.810443589231e-05\n",
      "Iter 7200, loss 9.83428253675811e-05\n",
      "Iter 7300, loss 9.870042413240299e-05\n",
      "Iter 7400, loss 9.893881360767409e-05\n",
      "Iter 7500, loss 9.929640509653836e-05\n",
      "Iter 7600, loss 9.953480184776708e-05\n",
      "Iter 7700, loss 9.989239333663136e-05\n",
      "Iter 7800, loss 0.00010024998482549563\n",
      "Iter 7900, loss 0.00010048838157672435\n",
      "Iter 8000, loss 0.00010084597306558862\n",
      "Iter 8100, loss 0.0001012035645544529\n",
      "Iter 8200, loss 0.00010144196130568162\n",
      "Iter 8300, loss 0.00010179955279454589\n",
      "Iter 8400, loss 0.00010215714428341016\n",
      "Iter 8500, loss 0.00010251473577227443\n",
      "Iter 8600, loss 0.00010275312524754554\n",
      "Iter 8700, loss 0.00010311071673640981\n",
      "Iter 8800, loss 0.00010346830822527409\n",
      "Iter 8900, loss 0.00010382589971413836\n",
      "Iter 9000, loss 0.00010418349120300263\n",
      "Iter 9100, loss 0.0001045410826918669\n"
     ]
    }
   ],
   "source": [
    "dist = attacks.avg_distance_to_errorset(test_net,\n",
    "                                 radius=1,\n",
    "                                 correct_classification=0,\n",
    "                                 n_pts=100,\n",
    "                                 max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
